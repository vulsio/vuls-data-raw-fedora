{
	"alias": "FEDORA-2018-b22d46eabb",
	"bugs": [
		{
			"bug_id": 1528396,
			"parent": true,
			"security": true,
			"title": "CVE-2018-5748 Libvirt: resource exhaustion via qemuMonitorIORead() method",
			"bugzilla": {
				"bug_id": "1528396",
				"alias": "CVE-2018-5748",
				"creation_ts": "2017-12-21 18:34:15 +0000",
				"short_desc": "CVE-2018-5748 libvirt: Resource exhaustion via qemuMonitorIORead() method",
				"delta_ts": "2021-02-17 01:04:21 +0000",
				"bug_status": "CLOSED",
				"resolution": "ERRATA",
				"keywords": "Reopened, Security",
				"priority": "low",
				"bug_severity": "low",
				"depends_on": [
					"1535785",
					"1550979",
					"1550980",
					"1566978"
				],
				"blocked": [
					{
						"bug_id": "1528397",
						"error": "NotPermitted"
					}
				],
				"external_bugs": {
					"text": "RHSA-2018:1929",
					"name": "Red Hat Product Errata"
				},
				"long_desc": [
					{
						"isprivate": "0",
						"commentid": "11130413",
						"comment_count": "0",
						"who": {
							"text": "psampaio",
							"name": "Pedro Sampaio"
						},
						"bug_when": "2017-12-21 18:34:15 +0000",
						"thetext": "A flaw was found in Qemu. A lack of restriction for the amount of data read by QEMU Monitor socket can lead to denial of service by exhaustion of memory resources.\n\nReferences:\n\nhttps://www.redhat.com/archives/libvir-list/2017-December/msg00749.html"
					},
					{
						"isprivate": "0",
						"commentid": "11130414",
						"comment_count": "1",
						"who": {
							"text": "psampaio",
							"name": "Pedro Sampaio"
						},
						"bug_when": "2017-12-21 18:34:29 +0000",
						"thetext": "Acknowledgments:\n\nName: Daniel P. Berrange (Red Hat), Peter Krempa (Red Hat)"
					},
					{
						"isprivate": "0",
						"commentid": "11194676",
						"comment_count": "2",
						"who": {
							"text": "ppandit",
							"name": "Prasad Pandit"
						},
						"bug_when": "2018-01-18 06:05:43 +0000",
						"thetext": "Created libvirt tracking bugs for this issue:\n\nAffects: fedora-all [bug 1535785]"
					},
					{
						"isprivate": "0",
						"commentid": "11214568",
						"comment_count": "3",
						"who": {
							"text": "cbuissar",
							"name": "Cedric Buissart"
						},
						"bug_when": "2018-01-24 14:36:48 +0000",
						"thetext": "Although RHES-3 (RHGS) is shipped with libvirt, it does not use Qemu. As such, there is no qemu process running, and no vulnerable monitor socket created."
					},
					{
						"isprivate": "0",
						"commentid": "11338845",
						"comment_count": "4",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-03-01 16:23:28 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
					},
					{
						"isprivate": "0",
						"commentid": "11594170",
						"comment_count": "6",
						"who": {
							"text": "errata-xmlrpc",
							"name": "errata-xmlrpc"
						},
						"bug_when": "2018-05-14 16:11:22 +0000",
						"thetext": "This issue has been addressed in the following products:\n\n  Red Hat Enterprise Linux 7\n\nVia RHSA-2018:1396 https://access.redhat.com/errata/RHSA-2018:1396"
					},
					{
						"isprivate": "0",
						"commentid": "11716303",
						"comment_count": "7",
						"who": {
							"text": "errata-xmlrpc",
							"name": "errata-xmlrpc"
						},
						"bug_when": "2018-06-19 04:55:50 +0000",
						"thetext": "This issue has been addressed in the following products:\n\n  Red Hat Enterprise Linux 6\n\nVia RHSA-2018:1929 https://access.redhat.com/errata/RHSA-2018:1929"
					}
				]
			}
		},
		{
			"bug_id": 1535785,
			"security": true,
			"title": "CVE-2018-5748 Libvirt: resource exhaustion via qemuMonitorIORead() method [fedora-all]",
			"bugzilla": {
				"bug_id": "1535785",
				"creation_ts": "2018-01-18 06:05:21 +0000",
				"short_desc": "CVE-2018-5748 Libvirt: resource exhaustion via qemuMonitorIORead() method [fedora-all]",
				"delta_ts": "2018-06-26 14:42:33 +0000",
				"bug_status": "CLOSED",
				"resolution": "CURRENTRELEASE",
				"keywords": "Security, SecurityTracking",
				"priority": "medium",
				"bug_severity": "medium",
				"blocked": [
					{
						"bug_id": "1528396",
						"alias": "CVE-2018-5748",
						"creation_ts": "2017-12-21 18:34:15 +0000",
						"short_desc": "CVE-2018-5748 libvirt: Resource exhaustion via qemuMonitorIORead() method",
						"delta_ts": "2021-02-17 01:04:21 +0000",
						"bug_status": "CLOSED",
						"resolution": "ERRATA",
						"keywords": "Reopened, Security",
						"priority": "low",
						"bug_severity": "low",
						"depends_on": [
							"1535785",
							"1550979",
							"1550980",
							"1566978"
						],
						"blocked": [
							{
								"bug_id": "1528397",
								"error": "NotPermitted"
							}
						],
						"external_bugs": {
							"text": "RHSA-2018:1929",
							"name": "Red Hat Product Errata"
						},
						"long_desc": [
							{
								"isprivate": "0",
								"commentid": "11130413",
								"comment_count": "0",
								"who": {
									"text": "psampaio",
									"name": "Pedro Sampaio"
								},
								"bug_when": "2017-12-21 18:34:15 +0000",
								"thetext": "A flaw was found in Qemu. A lack of restriction for the amount of data read by QEMU Monitor socket can lead to denial of service by exhaustion of memory resources.\n\nReferences:\n\nhttps://www.redhat.com/archives/libvir-list/2017-December/msg00749.html"
							},
							{
								"isprivate": "0",
								"commentid": "11130414",
								"comment_count": "1",
								"who": {
									"text": "psampaio",
									"name": "Pedro Sampaio"
								},
								"bug_when": "2017-12-21 18:34:29 +0000",
								"thetext": "Acknowledgments:\n\nName: Daniel P. Berrange (Red Hat), Peter Krempa (Red Hat)"
							},
							{
								"isprivate": "0",
								"commentid": "11194676",
								"comment_count": "2",
								"who": {
									"text": "ppandit",
									"name": "Prasad Pandit"
								},
								"bug_when": "2018-01-18 06:05:43 +0000",
								"thetext": "Created libvirt tracking bugs for this issue:\n\nAffects: fedora-all [bug 1535785]"
							},
							{
								"isprivate": "0",
								"commentid": "11214568",
								"comment_count": "3",
								"who": {
									"text": "cbuissar",
									"name": "Cedric Buissart"
								},
								"bug_when": "2018-01-24 14:36:48 +0000",
								"thetext": "Although RHES-3 (RHGS) is shipped with libvirt, it does not use Qemu. As such, there is no qemu process running, and no vulnerable monitor socket created."
							},
							{
								"isprivate": "0",
								"commentid": "11338845",
								"comment_count": "4",
								"who": {
									"text": "updates",
									"name": "Fedora Update System"
								},
								"bug_when": "2018-03-01 16:23:28 +0000",
								"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
							},
							{
								"isprivate": "0",
								"commentid": "11594170",
								"comment_count": "6",
								"who": {
									"text": "errata-xmlrpc",
									"name": "errata-xmlrpc"
								},
								"bug_when": "2018-05-14 16:11:22 +0000",
								"thetext": "This issue has been addressed in the following products:\n\n  Red Hat Enterprise Linux 7\n\nVia RHSA-2018:1396 https://access.redhat.com/errata/RHSA-2018:1396"
							},
							{
								"isprivate": "0",
								"commentid": "11716303",
								"comment_count": "7",
								"who": {
									"text": "errata-xmlrpc",
									"name": "errata-xmlrpc"
								},
								"bug_when": "2018-06-19 04:55:50 +0000",
								"thetext": "This issue has been addressed in the following products:\n\n  Red Hat Enterprise Linux 6\n\nVia RHSA-2018:1929 https://access.redhat.com/errata/RHSA-2018:1929"
							}
						]
					}
				],
				"long_desc": [
					{
						"isprivate": "0",
						"commentid": "11194673",
						"comment_count": "0",
						"who": {
							"text": "ppandit",
							"name": "Prasad Pandit"
						},
						"bug_when": "2018-01-18 06:05:21 +0000",
						"thetext": "\nThis is an automatically created tracking bug!  It was created to ensure\nthat one or more security vulnerabilities are fixed in affected versions\nof fedora-all.\n\nFor comments that are specific to the vulnerability please use bugs filed\nagainst the \"Security Response\" product referenced in the \"Blocks\" field.\n\nFor more information see:\nhttp://fedoraproject.org/wiki/Security/TrackingBugs\n\nWhen submitting as an update, use the fedpkg template provided in the next\ncomment(s).  This will include the bug IDs of this tracking bug as well as\nthe relevant top-level CVE bugs.\n\nPlease also mention the CVE IDs being fixed in the RPM changelog and the\nfedpkg commit message.\n\nNOTE: this issue affects multiple supported versions of Fedora. While only\none tracking bug has been filed, please correct all affected versions at\nthe same time.  If you need to fix the versions independent of each other,\nyou may clone this bug as appropriate."
					},
					{
						"isprivate": "0",
						"commentid": "11194674",
						"comment_count": "1",
						"who": {
							"text": "ppandit",
							"name": "Prasad Pandit"
						},
						"bug_when": "2018-01-18 06:05:34 +0000",
						"thetext": "Use the following template to for the 'fedpkg update' request to submit an\nupdate for this issue as it contains the top-level parent bug(s) as well as\nthis tracking bug.  This will ensure that all associated bugs get updated\nwhen new packages are pushed to stable.\n\n=====\n\n# bugfix, security, enhancement, newpackage (required)\ntype=security\n\n# testing, stable\nrequest=testing\n\n# Bug numbers: 1234,9876\nbugs=1528396,1535785\n\n# Description of your update\nnotes=Security fix for [PUT CVEs HERE]\n\n# Enable request automation based on the stable/unstable karma thresholds\nautokarma=True\nstable_karma=3\nunstable_karma=-3\n\n# Automatically close bugs when this marked as stable\nclose_bugs=True\n\n# Suggest that users restart after update\nsuggest_reboot=False\n\n======\n\nAdditionally, you may opt to use the bodhi web interface to submit updates:\n\nhttps://bodhi.fedoraproject.org/updates/new"
					},
					{
						"isprivate": "0",
						"commentid": "11280681",
						"comment_count": "2",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-02-13 20:32:31 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been submitted as an update to Fedora 27. https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb"
					},
					{
						"isprivate": "0",
						"commentid": "11284062",
						"comment_count": "3",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-02-14 18:27:55 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 testing repository. If problems still persist, please make note of it in this bug report.\nSee https://fedoraproject.org/wiki/QA:Updates_Testing for\ninstructions on how to install test updates.\nYou can provide feedback for this update here: https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb"
					},
					{
						"isprivate": "0",
						"commentid": "11338842",
						"comment_count": "4",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-03-01 16:23:12 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
					}
				]
			}
		},
		{
			"bug_id": 1540872,
			"title": "Hotplug disk failing due to cgroups",
			"bugzilla": {
				"bug_id": "1540872",
				"creation_ts": "2018-02-01 08:53:34 +0000",
				"short_desc": "Hotplug disk failing due to cgroups",
				"delta_ts": "2018-06-26 14:42:42 +0000",
				"bug_status": "CLOSED",
				"resolution": "CURRENTRELEASE",
				"priority": "unspecified",
				"bug_severity": "unspecified",
				"blocked": [
					{
						"bug_id": "1469235",
						"creation_ts": "2017-07-10 17:00:45 +0000",
						"short_desc": "[Blocked on platform bug 1532183] Hotplug failed because libvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk7' could not be initialized",
						"delta_ts": "2018-07-04 22:41:00 +0000",
						"bug_status": "CLOSED",
						"resolution": "CURRENTRELEASE",
						"keywords": "Automation",
						"priority": "unspecified",
						"bug_severity": "medium",
						"depends_on": [
							"1532183",
							"1540872"
						],
						"blocked": [
							{
								"bug_id": "1526197",
								"creation_ts": "2017-12-14 22:36:41 +0000",
								"short_desc": "Hotplug failed because libvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk7' could not be initialized",
								"delta_ts": "2021-12-10 15:29:41 +0000",
								"bug_status": "CLOSED",
								"resolution": "ERRATA",
								"keywords": "Automation, ZStream",
								"priority": "high",
								"bug_severity": "high",
								"depends_on": [
									"1469235",
									"1532183"
								],
								"blocked": [
									{
										"bug_id": "1547358",
										"creation_ts": "2018-02-21 04:39:06 +0000",
										"short_desc": "Unable to detach the disk while running commvault backup proxy.",
										"delta_ts": "2019-05-16 13:08:30 +0000",
										"bug_status": "CLOSED",
										"resolution": "ERRATA",
										"keywords": "ZStream",
										"priority": "unspecified",
										"bug_severity": "high",
										"depends_on": [
											"1526197"
										],
										"blocked": [
											{
												"bug_id": "1554289",
												"creation_ts": "2018-03-12 10:51:11 +0000",
												"short_desc": "[downstream clone - 4.1.10] Unable to detach the disk while running commvault backup proxy.",
												"delta_ts": "2019-05-16 13:06:46 +0000",
												"bug_status": "CLOSED",
												"resolution": "ERRATA",
												"keywords": "ZStream",
												"priority": "unspecified",
												"bug_severity": "high",
												"depends_on": [
													"1547358"
												],
												"external_bugs": {
													"text": "RHEA-2018:0563",
													"name": "Red Hat Product Errata"
												},
												"long_desc": [
													{
														"isprivate": "0",
														"commentid": "11369034",
														"comment_count": "0",
														"who": {
															"text": "rhv-bugzilla-bot",
															"name": "RHV bug bot"
														},
														"bug_when": "2018-03-12 10:51:11 +0000",
														"thetext": "+++ This bug is a downstream clone. The original bug is: +++\n+++   bug 1547358 +++\n======================================================================\n\nDescription of problem:\nUnable to detach the disks from the backup proxy. Though the manual detaches works. \n\nVersion-Release number of selected component (if applicable):\nRHV 4.1.9\n\nHow reproducible:\nintermittent\n\nSteps to Reproduce:\nBackup proxy script tries to detach the disk and fails with following reasons as per the logs: \n\n - Cannot detach Virtual Disk because the VM is in Paused status.\n - Cannot remove Virtual Disk. At least one of the VMs is not down.\n\nActual results:\nAuto Detach of disks fails. \n\nExpected results:\nDetach should be successful. \n\nAdditional info:\nAttaching log from commvault.\n\n(Originally by Ulhas Surse)"
													},
													{
														"isprivate": "0",
														"commentid": "11369038",
														"comment_count": "4",
														"who": {
															"text": "rhv-bugzilla-bot",
															"name": "RHV bug bot"
														},
														"bug_when": "2018-03-12 10:51:27 +0000",
														"thetext": "This is probably another libvirt issue (or a different facet of the same issue) around the same area.\nLet's revisit once we have the original BZ fixed.\n\n(Originally by Allon Mureinik)"
													},
													{
														"isprivate": "0",
														"commentid": "11379155",
														"comment_count": "5",
														"who": {
															"text": "rhv-bugzilla-bot",
															"name": "RHV bug bot"
														},
														"bug_when": "2018-03-14 16:45:49 +0000",
														"thetext": "WARN: Bug status wasn't changed from MODIFIED to ON_QA due to the following reason:\n\n[Found non-acked flags: '{'rhevm-4.1.z': '?'}', ]\n\nFor more info please contact: rhv-devops@redhat.comINFO: Bug status wasn't changed from MODIFIED to ON_QA due to the following reason:\n\n[Found non-acked flags: '{'rhevm-4.1.z': '?'}', ]\n\nFor more info please contact: rhv-devops@redhat.com"
													},
													{
														"isprivate": "0",
														"commentid": "11386934",
														"comment_count": "6",
														"who": {
															"text": "ebenahar",
															"name": "Elad"
														},
														"bug_when": "2018-03-16 12:02:56 +0000",
														"thetext": "[root@storage-ge7-vdsm1 ~]# repoquery --requires vdsm |grep libvirt-daemon-kvm\nlibvirt-daemon-kvm >= 3.2.0-14.el7_4.8\n\n\n\nUsed:\nvdsm-4.19.50-1.el7ev.x86_64\nRHEL7.5"
													},
													{
														"isprivate": "0",
														"commentid": "11399260",
														"comment_count": "9",
														"who": {
															"text": "errata-xmlrpc",
															"name": "errata-xmlrpc"
														},
														"bug_when": "2018-03-20 16:38:36 +0000",
														"thetext": "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHEA-2018:0563"
													},
													{
														"isprivate": "0",
														"commentid": "12743543",
														"comment_count": "10",
														"who": {
															"text": "fkust",
															"name": "Franta Kust"
														},
														"bug_when": "2019-05-16 13:06:46 +0000",
														"thetext": "BZ<2>Jira Resync"
													}
												]
											}
										],
										"external_bugs": {
											"text": "RHEA-2018:1489",
											"name": "Red Hat Product Errata"
										},
										"long_desc": [
											{
												"isprivate": "0",
												"commentid": "11303691",
												"comment_count": "0",
												"who": {
													"text": "usurse",
													"name": "Ulhas Surse"
												},
												"bug_when": "2018-02-21 04:39:06 +0000",
												"thetext": "Description of problem:\nUnable to detach the disks from the backup proxy. Though the manual detaches works. \n\nVersion-Release number of selected component (if applicable):\nRHV 4.1.9\n\nHow reproducible:\nintermittent\n\nSteps to Reproduce:\nBackup proxy script tries to detach the disk and fails with following reasons as per the logs: \n\n - Cannot detach Virtual Disk because the VM is in Paused status.\n - Cannot remove Virtual Disk. At least one of the VMs is not down.\n\nActual results:\nAuto Detach of disks fails. \n\nExpected results:\nDetach should be successful. \n\nAdditional info:\nAttaching log from commvault."
											},
											{
												"isprivate": "0",
												"commentid": "11310755",
												"comment_count": "3",
												"who": {
													"text": "amureini",
													"name": "Allon Mureinik"
												},
												"bug_when": "2018-02-22 10:09:00 +0000",
												"thetext": "This is probably another libvirt issue (or a different facet of the same issue) around the same area.\nLet's revisit once we have the original BZ fixed."
											},
											{
												"isprivate": "0",
												"commentid": "11386970",
												"comment_count": "5",
												"who": {
													"text": "ebenahar",
													"name": "Elad"
												},
												"bug_when": "2018-03-16 12:19:50 +0000",
												"thetext": "[root@storage-ge6-vdsm1 ~]# repoquery --requires vdsm |grep libvirt-daemon-kvm\nlibvirt-daemon-kvm >= 3.2.0-14.el7_4.8\n\nUsed:\nvdsm-4.20.22-1.el7ev.x86_64"
											},
											{
												"isprivate": "0",
												"commentid": "11600588",
												"comment_count": "10",
												"who": {
													"text": "errata-xmlrpc",
													"name": "errata-xmlrpc"
												},
												"bug_when": "2018-05-15 17:54:02 +0000",
												"thetext": "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHEA-2018:1489"
											},
											{
												"isprivate": "0",
												"commentid": "12743774",
												"comment_count": "11",
												"who": {
													"text": "fkust",
													"name": "Franta Kust"
												},
												"bug_when": "2019-05-16 13:08:30 +0000",
												"thetext": "BZ<2>Jira Resync"
											}
										]
									},
									{
										"bug_id": "1554290",
										"creation_ts": "2018-03-12 10:51:44 +0000",
										"short_desc": "[downstream clone - 4.1.10] Hotplug failed because libvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk7' could not be initialized",
										"delta_ts": "2021-12-10 15:47:25 +0000",
										"bug_status": "CLOSED",
										"resolution": "ERRATA",
										"keywords": "Automation, ZStream",
										"priority": "high",
										"bug_severity": "high",
										"depends_on": [
											"1526197"
										],
										"external_bugs": {
											"text": "RHEA-2018:0563",
											"name": "Red Hat Product Errata"
										},
										"long_desc": [
											{
												"isprivate": "0",
												"commentid": "11369043",
												"comment_count": "1",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:52:09 +0000",
												"thetext": "These versions also reproduce the issue:\n\nlibvirt-daemon-kvm-3.2.0-14.el7_4.3.x86_64\nsystemd-219-42.el7_4.4.x86_64\nvdsm-4.19.36-1.el7ev.x86_64\n\nWe also have report that the operation succeeds in case the backup vm (commvault) and the vm being snapshoted run on the same host.\n\nRaz, somehow I can't reproduce this. Could you please give me access to your reproduction env? \nI assume we can get a better idea on where the EPERM comes from using stap/ftrace.\n\nWhen attaching the snapshot to the backup VM, it looks like qemu is able to open the first image of the chain, and the subsequent one fails:\n\nCould not open backing file: Could not open '/rhev/data-center/58bdc405-028f-00dc-0026-0000000000aa/a3a4dfce-ca1b-4453-a40b-dc43c4f2191a/images/1c28bd60-8556-40fd-8229-f52bdd51ca7a/25165986-adaa-4454-a7de-2d757b34e8f5': Operation not permitted\n\n(Originally by Germano Veit Michel)"
											},
											{
												"isprivate": "0",
												"commentid": "11369046",
												"comment_count": "3",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:52:21 +0000",
												"thetext": "(In reply to Germano Veit Michel from comment #1)\n> These versions also reproduce the issue:\n> \n> libvirt-daemon-kvm-3.2.0-14.el7_4.3.x86_64\n> systemd-219-42.el7_4.4.x86_64\n> vdsm-4.19.36-1.el7ev.x86_64\n> \n> We also have report that the operation succeeds in case the backup vm\n> (commvault) and the vm being snapshoted run on the same host.\n> \n> Raz, somehow I can't reproduce this. Could you please give me access to your\n> reproduction env? \nI'm not sure you are trying the same scenario mentioned in the original bug\nWhat are the steps you are doing?\n> I assume we can get a better idea on where the EPERM comes from using\n> stap/ftrace.\n> \n> When attaching the snapshot to the backup VM, it looks like qemu is able to\n> open the first image of the chain, and the subsequent one fails:\n> \n> Could not open backing file: Could not open\n> '/rhev/data-center/58bdc405-028f-00dc-0026-0000000000aa/a3a4dfce-ca1b-4453-\n> a40b-dc43c4f2191a/images/1c28bd60-8556-40fd-8229-f52bdd51ca7a/25165986-adaa-\n> 4454-a7de-2d757b34e8f5': Operation not permitted\n\n(Originally by Raz Tamir)"
											},
											{
												"isprivate": "0",
												"commentid": "11369048",
												"comment_count": "5",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:52:45 +0000",
												"thetext": "(In reply to Raz Tamir from comment #2)\n> I'm not sure you are trying the same scenario mentioned in the original bug\n> What are the steps you are doing?\n\nI'm using the backup api script in a loop (similar to what Commvault and our customers do). But all the snapshot attachments work. What are you using? This?\n\n> On VM with 4 disks - A, B, C, D where A is the bootable disk with OS. After\n> starting the VM, if we will hot-unplug the 3 disks in order B, C and D and\n> after that will try to hotplug back disk C, it will fail with:\n\n(Originally by Germano Veit Michel)"
											},
											{
												"isprivate": "0",
												"commentid": "11369050",
												"comment_count": "6",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:52:59 +0000",
												"thetext": "(In reply to Germano Veit Michel from comment #4)\n> (In reply to Raz Tamir from comment #2)\n> > I'm not sure you are trying the same scenario mentioned in the original bug\n> > What are the steps you are doing?\n> \n> I'm using the backup api script in a loop (similar to what Commvault and our\n> customers do). But all the snapshot attachments work. What are you using?\n> This?\n> \n> > On VM with 4 disks - A, B, C, D where A is the bootable disk with OS. After\n> > starting the VM, if we will hot-unplug the 3 disks in order B, C and D and\n> > after that will try to hotplug back disk C, it will fail with:\n\nYes,\n\nYou can try step 1 - 3 and only then, use the backup api method and attached the snapshot disk.  I think it will trigger the issue\n\n(Originally by Raz Tamir)"
											},
											{
												"isprivate": "0",
												"commentid": "11369051",
												"comment_count": "7",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:53:11 +0000",
												"thetext": "(In reply to Raz Tamir from comment #5)\n> Yes,\n> \n> You can try step 1 - 3 and only then, use the backup api method and attached\n> the snapshot disk.  I think it will trigger the issue\n\nThanks Raz, I was finally able to reproduce it.\n\nsystemd-219-42.el7_4.4.x86_64\nlibvirt-daemon-kvm-3.2.0-14.el7_4.5.x86_64\nvdsm-4.19.42-1.el7ev.x86_64\nkernel-3.10.0-693.11.1.el7.x86_64\nqemu-kvm-rhev-2.9.0-16.el7_4.11.x86_64\n\nIt seems only reproducible on Block Storage (on NFS I could not).\n\nThese are the conditions I found:\nVM with 4 disks, A, B, C, D\nBoot\nUnplug B, C an D\nAll hotplugs will fail unless I hotplug B. If B is plugged, the other 2 seem to work.\nMay not be 100% accurate, but it's what I found after playing with it.\n\nMoving forward looks like the problem appears for qemu with a open syscall returning EPERM, doesn't seem related to SELinux.\n\n# getenforce \nPermissive\n\n# strace -p 3521 -e trace=open\nstrace: Process 3521 attached\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDONLY|O_NONBLOCK|O_CLOEXEC) = -1 EPERM (Operation not permitted)\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDWR|O_DIRECT|O_CLOEXEC) = -1 EPERM (Operation not permitted)\n\nSo open returns -1. It's a bit weird that it's a EPERM (not ACCESS).\n\nThen I created an audit rule to track \"open\", capture both a success and a failure opening the same disk:\n# auditctl -a always,exit -F arch=b64 -F pid=3521 -S open -k qemu-open\n\nFAIL:\ntime->Thu Dec 21 14:30:27 2017\ntype=PROCTITLE msg=audit(1513830627.873:3248): proctitle=2F7573722F6C6962657865632F71656D752D6B766D002D6E616D650067756573743D54657374564D2C64656275672D746872656164733D6F6E002D53002D6F626A656374007365637265742C69643D6D61737465724B6579302C666F726D61743D7261772C66696C653D2F7661722F6C69622F6C6962766972742F71656D752F\ntype=PATH msg=audit(1513830627.873:3248): item=0 name=\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\" inode=53420 dev=00:27 mode=060660 ouid=36 ogid=107 rdev=fc:09 obj=system_u:object_r:svirt_image_t:s0:c83,c485 objtype=NORMAL\ntype=CWD msg=audit(1513830627.873:3248):  cwd=\"/\"\ntype=SYSCALL msg=audit(1513830627.873:3248): arch=c000003e syscall=2 success=no exit=-1 a0=5638be345340 a1=84002 a2=0 a3=7fd3d2803b00 items=1 ppid=1 pid=3521 auid=4294967295 uid=107 gid=107 euid=107 suid=107 fsuid=107 egid=107 sgid=107 fsgid=107 tty=(none) ses=4294967295 comm=\"qemu-kvm\" exe=\"/usr/libexec/qemu-kvm\" subj=system_u:system_r:svirt_t:s0:c83,c485 key=\"qemu-open\"\n\nSUCCESS:\ntime->Thu Dec 21 14:31:41 2017\ntype=PROCTITLE msg=audit(1513830701.500:3321): proctitle=2F7573722F6C6962657865632F71656D752D6B766D002D6E616D650067756573743D54657374564D2C64656275672D746872656164733D6F6E002D53002D6F626A656374007365637265742C69643D6D61737465724B6579302C666F726D61743D7261772C66696C653D2F7661722F6C69622F6C6962766972742F71656D752F\ntype=PATH msg=audit(1513830701.500:3321): item=0 name=\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\" inode=53420 dev=00:27 mode=060660 ouid=36 ogid=107 rdev=fc:09 obj=system_u:object_r:svirt_image_t:s0:c83,c485 objtype=NORMAL\ntype=CWD msg=audit(1513830701.500:3321):  cwd=\"/\"\ntype=SYSCALL msg=audit(1513830701.500:3321): arch=c000003e syscall=2 success=yes exit=77 a0=5638be346730 a1=80800 a2=0 a3=ffffff80 items=1 ppid=1 pid=3521 auid=4294967295 uid=107 gid=107 euid=107 suid=107 fsuid=107 egid=107 sgid=107 fsgid=107 tty=(none) ses=4294967295 comm=\"qemu-kvm\" exe=\"/usr/libexec/qemu-kvm\" subj=system_u:system_r:svirt_t:s0:c83,c485 key=\"qemu-open\"\n\nIt seems everything is exactly the same? One succeeds and one fails?\n\nI tried to stap to try to get some more info, but couldn't get far enough with my current stap, pretty much just confirmed the problem:\n\nprobe kernel.function(\"sys_open\").return {\n    if (pid() != 2090)\n        next\n    if ($return != -1)\n        next\n    printf(\"open returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))    \n    next\n}\n\nopen returned -1, performed by qemu-kvm, pid 2038\n  filename=0x55beedebdad0 flags=0x80800 mode=0x0\nopen returned -1, performed by qemu-kvm, pid 2038\n  filename=0x55beedebdad0 flags=0x84002 mode=0x0\n\nI would get to get a \"forward trace\" of all the functions sys_open called to get an idea where the -1 is coming from. But with a return probe, that is not going to happen.\nAnd ftrace is showing too much data (mostly selinux stuff), pretty much unreadable.\n\nAnd this is where I stopped today. Any ideas how to move forward? I think I'll try downgrading some packages tomorrow. Looking at the customer cases, this seem to have started with 7.4?\n\n(Originally by Germano Veit Michel)"
											},
											{
												"isprivate": "0",
												"commentid": "11369053",
												"comment_count": "8",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:53:26 +0000",
												"thetext": "Tracked it down to cgroups within the kernel:\n\n# cat qemu_open.stp \nprobe kernel.function(\"do_sys_open\").return {\n    if (pid() != 2632) \n        next\n    if ($return != -1)\n        next\n    printf(\"do_sys_open returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))\n}\n\nprobe kernel.function(\"__devcgroup_check_permission\").return {\n    if (pid() != 2632) \n        next\n    if ($return != -1)\n        next\n    printf(\"__devcgroup_check_permission returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))\n}\n\nDuring failure, we have this:\n\n# stap qemu_open.stp \n__devcgroup_check_permission returned -1, performed by qemu-kvm, pid 2632\n  type=0x1 major=0xfc minor=0x9 access=0x2\ndo_sys_open returned -1, performed by qemu-kvm, pid 2632\n  dfd=0xffffffffffffff9c filename=0x56354c7dfa20 flags=0x88800 mode=0x0\n__devcgroup_check_permission returned -1, performed by qemu-kvm, pid 2632\n  type=0x1 major=0xfc minor=0x9 access=0x6\ndo_sys_open returned -1, performed by qemu-kvm, pid 2632\n  dfd=0xffffffffffffff9c filename=0x56354c7dfa20 flags=0x8c002 mode=0x0\n\nSo __devcgroup_check_permission generates the EPERM, which propagates all the way back to sys_do_open, and qemu-kvm gets this:\n\n# strace -p 2632 -e trace=open\nstrace: Process 2632 attached\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDONLY|O_NONBLOCK|O_CLOEXEC) = -1 EPERM (Operation not permitted)\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDWR|O_DIRECT|O_CLOEXEC) = -1 EPERM (Operation not permitted)\n\nHere an idea on what happens within the kernel at that point:\n\n\nqemu-kvm-2632  [000]   816.223260: funcgraph_entry:            |            may_open() {\nqemu-kvm-2632  [000]   816.223260: funcgraph_entry:            |              inode_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry:            |                __inode_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry:            |                  generic_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry: 0.038 us   |                    get_acl();\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry: 0.043 us   |                    in_group_p();\nqemu-kvm-2632  [000]   816.223261: funcgraph_exit:  0.557 us   |                  }\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry:            |                  __devcgroup_inode_permission() {\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry:            |                    __devcgroup_check_permission() {\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry: 0.317 us   |                      match_exception();\nqemu-kvm-2632  [000]   816.223262: funcgraph_exit:  0.597 us   |                    }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  0.856 us   |                  }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.018 us   |                }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.339 us   |              }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.636 us   |            }\n\nqemu-kvm-2632  [000]   816.223272: sys_exit_open:        0xffffffffffffffff\n\nSo, looks like a libvirt bug with cgroups?\n\nI'm going on PTO for 2 weeks and wont be able to continue this. But I assume the next step would be to track cgroup manipulation within libvirt or involve libvirt devel.\n\n(Originally by Germano Veit Michel)"
											},
											{
												"isprivate": "0",
												"commentid": "11369055",
												"comment_count": "10",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:53:49 +0000",
												"thetext": "Is there a sosreport from a host after the failure is reproduced, somewhere? I would like to take a look at the croup data.\n\n(Originally by Eduardo Habkost)"
											},
											{
												"isprivate": "0",
												"commentid": "11369057",
												"comment_count": "12",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:54:13 +0000",
												"thetext": "I just found out that the audit logs are broken due to a libvirt bug. See https://github.com/ehabkost/libvirt/commit/fa7b97da69595ec4b8992ccaacfe5a7347436d6a\n\n(Originally by Eduardo Habkost)"
											},
											{
												"isprivate": "0",
												"commentid": "11369058",
												"comment_count": "13",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:54:27 +0000",
												"thetext": "I need help from somebody familiar with VDSM to reproduce the bug. Is there anybody available this week who is able to reproduce the bug and can give me access to the host where it can be seen?\n\n(Originally by Eduardo Habkost)"
											},
											{
												"isprivate": "0",
												"commentid": "11369060",
												"comment_count": "15",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:54:47 +0000",
												"thetext": "Good news: I think I have just reproduced it using libvirt\ndirectly, after activating/deactivating the LVM volumes on every\nattach/detach operation.  I think the bug is caused by block\ndevice major/minor number reuse when the devices are re-enabled\nin a different order.\n\nI will investigate further and report back soon.\n\n(Originally by Eduardo Habkost)"
											},
											{
												"isprivate": "0",
												"commentid": "11369062",
												"comment_count": "16",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:54:59 +0000",
												"thetext": "I confirm that this a bug on libvirt's namespace handling.  It's possible to work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf.\n\n(Originally by Eduardo Habkost)"
											},
											{
												"isprivate": "0",
												"commentid": "11369064",
												"comment_count": "17",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:55:11 +0000",
												"thetext": "Experimental fix at:\nhttps://github.com/ehabkost/libvirt/commit/89f1a08b9518148f6a86600c0ded6f52886e44b4\n\n(Originally by Eduardo Habkost)"
											},
											{
												"isprivate": "0",
												"commentid": "11369065",
												"comment_count": "18",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:55:23 +0000",
												"thetext": "(In reply to Eduardo Habkost from comment #15)\n> I confirm that this a bug on libvirt's namespace handling.  It's possible to\n> work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf.\n\nThe suggested W/A worked for me:\n\n1) added namespaces = [ ] to /etc/libvirt/qemu.conf\n2) # systemctl restart vdsmd\n3) # systemctl restart libvirtd\n4) and performed the steps to reproduce from the original bug.\n\n(Originally by Raz Tamir)"
											},
											{
												"isprivate": "0",
												"commentid": "11369066",
												"comment_count": "19",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:55:38 +0000",
												"thetext": "A proper fix depends on a libvirt fix that is not yet released. Pushing out to 4.1.10.\n\n(Originally by Allon Mureinik)"
											},
											{
												"isprivate": "0",
												"commentid": "11369070",
												"comment_count": "23",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:56:23 +0000",
												"thetext": "This bug is no longer depends on platform side as all depended bug was addressed.\n\nDaniel,\n\nCan you take a look at this?\n\n(Originally by Raz Tamir)"
											},
											{
												"isprivate": "0",
												"commentid": "11369072",
												"comment_count": "24",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:56:36 +0000",
												"thetext": "(In reply to Raz Tamir from comment #22)\n> This bug is no longer depends on platform side as all depended bug was\n> addressed.\n> \n> Daniel,\n> \n> Can you take a look at this?\n\nThe libvirt build containing the fix was not released yet.\nRaz, did you test with an unreleased libvirt referenced in the platform bug? Could you clarify the above statement?\n\n(Originally by Allon Mureinik)"
											},
											{
												"isprivate": "0",
												"commentid": "11369073",
												"comment_count": "25",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:56:50 +0000",
												"thetext": "(In reply to Allon Mureinik from comment #23)\n> (In reply to Raz Tamir from comment #22)\n> > This bug is no longer depends on platform side as all depended bug was\n> > addressed.\n> > \n> > Daniel,\n> > \n> > Can you take a look at this?\n> \n> The libvirt build containing the fix was not released yet.\n> Raz, did you test with an unreleased libvirt referenced in the platform bug?\n> Could you clarify the above statement?\n\nHi Allon,\n\nBoth 'depends on' bugs are 'VERIFIED'.\nWe are currently working with the latest snapshot from platform before GA.\nWhen should we get it?\n\n(Originally by Raz Tamir)"
											},
											{
												"isprivate": "0",
												"commentid": "11369074",
												"comment_count": "26",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:57:03 +0000",
												"thetext": "(In reply to Raz Tamir from comment #24)\n> (In reply to Allon Mureinik from comment #23)\n> > (In reply to Raz Tamir from comment #22)\n> > > This bug is no longer depends on platform side as all depended bug was\n> > > addressed.\n> > > \n> > > Daniel,\n> > > \n> > > Can you take a look at this?\n> > \n> > The libvirt build containing the fix was not released yet.\n> > Raz, did you test with an unreleased libvirt referenced in the platform bug?\n> > Could you clarify the above statement?\n> \n> Hi Allon,\n> \n> Both 'depends on' bugs are 'VERIFIED'.\n> We are currently working with the latest snapshot from platform before GA.\n> When should we get it?\n\nI think we may be muddling two different things here.\n\nFrom a QE perspective - testing with pre-release libvirt bits is the right way to go, IMHO. If the newer libvirt solves the issue - great; if not - the problem should be escalated to libvirt's devs.\n\nFrom a RHV dev perspective - there's no AI on RHV's code. The only AI we have is to require the newer libvirt version that contains the fix in vdsm's spec file (and even that is just a nicety - you can always upgrade libvirt yourself and gain the benefits of the newer version regardless of vdsm's requirements). We cannot update vdsm's spec to require a libvirt version that isn't available yet, as this will break the ability to install vdsm.\n\nAccording to libvirt's 7.4.z bug, bz#1532183, their fix will be delivered by erratum RHBA-2018:32240-04. It is currently scheduled for March 6th.\n\n(Originally by Allon Mureinik)"
											},
											{
												"isprivate": "0",
												"commentid": "11369075",
												"comment_count": "27",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-12 10:57:16 +0000",
												"thetext": "Thanks for clarification Allon\n\n(Originally by Raz Tamir)"
											},
											{
												"isprivate": "0",
												"commentid": "11379156",
												"comment_count": "31",
												"who": {
													"text": "rhv-bugzilla-bot",
													"name": "RHV bug bot"
												},
												"bug_when": "2018-03-14 16:45:53 +0000",
												"thetext": "WARN: Bug status wasn't changed from MODIFIED to ON_QA due to the following reason:\n\n[Found non-acked flags: '{'rhevm-4.1.z': '?'}', ]\n\nFor more info please contact: rhv-devops@redhat.comINFO: Bug status wasn't changed from MODIFIED to ON_QA due to the following reason:\n\n[Found non-acked flags: '{'rhevm-4.1.z': '?'}', ]\n\nFor more info please contact: rhv-devops@redhat.com"
											},
											{
												"isprivate": "0",
												"commentid": "11382848",
												"comment_count": "32",
												"who": {
													"text": "ykaul",
													"name": "Yaniv Kaul"
												},
												"bug_when": "2018-03-15 11:18:49 +0000",
												"thetext": "Allon, can you see if this should move to ON_QA?"
											},
											{
												"isprivate": "0",
												"commentid": "11383426",
												"comment_count": "33",
												"who": {
													"text": "amureini",
													"name": "Allon Mureinik"
												},
												"bug_when": "2018-03-15 13:16:40 +0000",
												"thetext": "(In reply to Yaniv Kaul from comment #32)\n> Allon, can you see if this should move to ON_QA?\nYes.\nThe original bug was targeted to 4.1.10 and had all the acks, but the cloning process lost it. \nTL;DR, it's fixed in 4.19.50"
											},
											{
												"isprivate": "0",
												"commentid": "11384086",
												"comment_count": "34",
												"who": {
													"text": "ratamir",
													"name": "Raz Tamir"
												},
												"bug_when": "2018-03-15 15:22:41 +0000",
												"thetext": "Verified on rhvm-4.1.10.3-0.1.el7\n\nFollowed the steps to reproduce - Pass"
											},
											{
												"isprivate": "0",
												"commentid": "11399261",
												"comment_count": "37",
												"who": {
													"text": "errata-xmlrpc",
													"name": "errata-xmlrpc"
												},
												"bug_when": "2018-03-20 16:38:36 +0000",
												"thetext": "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHEA-2018:0563"
											},
											{
												"isprivate": "0",
												"commentid": "12743910",
												"comment_count": "38",
												"who": {
													"text": "fkust",
													"name": "Franta Kust"
												},
												"bug_when": "2019-05-16 13:09:32 +0000",
												"thetext": "BZ<2>Jira Resync"
											},
											{
												"isprivate": "0",
												"commentid": "13114832",
												"comment_count": "39",
												"who": {
													"text": "dagur",
													"name": "Daniel Gur"
												},
												"bug_when": "2019-08-28 13:15:15 +0000",
												"thetext": "sync2jira"
											},
											{
												"isprivate": "0",
												"commentid": "13115331",
												"comment_count": "40",
												"who": {
													"text": "dagur",
													"name": "Daniel Gur"
												},
												"bug_when": "2019-08-28 13:20:18 +0000",
												"thetext": "sync2jira"
											}
										]
									}
								],
								"external_bugs": {
									"text": "RHEA-2018:1489",
									"name": "Red Hat Product Errata"
								},
								"long_desc": [
									{
										"isprivate": "0",
										"commentid": "11110961",
										"comment_count": "1",
										"who": {
											"text": "gveitmic",
											"name": "Germano Veit Michel"
										},
										"bug_when": "2017-12-14 22:41:29 +0000",
										"thetext": "These versions also reproduce the issue:\n\nlibvirt-daemon-kvm-3.2.0-14.el7_4.3.x86_64\nsystemd-219-42.el7_4.4.x86_64\nvdsm-4.19.36-1.el7ev.x86_64\n\nWe also have report that the operation succeeds in case the backup vm (commvault) and the vm being snapshoted run on the same host.\n\nRaz, somehow I can't reproduce this. Could you please give me access to your reproduction env? \nI assume we can get a better idea on where the EPERM comes from using stap/ftrace.\n\nWhen attaching the snapshot to the backup VM, it looks like qemu is able to open the first image of the chain, and the subsequent one fails:\n\nCould not open backing file: Could not open '/rhev/data-center/58bdc405-028f-00dc-0026-0000000000aa/a3a4dfce-ca1b-4453-a40b-dc43c4f2191a/images/1c28bd60-8556-40fd-8229-f52bdd51ca7a/25165986-adaa-4454-a7de-2d757b34e8f5': Operation not permitted"
									},
									{
										"isprivate": "0",
										"commentid": "11121921",
										"comment_count": "2",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2017-12-19 12:02:45 +0000",
										"thetext": "(In reply to Germano Veit Michel from comment #1)\n> These versions also reproduce the issue:\n> \n> libvirt-daemon-kvm-3.2.0-14.el7_4.3.x86_64\n> systemd-219-42.el7_4.4.x86_64\n> vdsm-4.19.36-1.el7ev.x86_64\n> \n> We also have report that the operation succeeds in case the backup vm\n> (commvault) and the vm being snapshoted run on the same host.\n> \n> Raz, somehow I can't reproduce this. Could you please give me access to your\n> reproduction env? \nI'm not sure you are trying the same scenario mentioned in the original bug\nWhat are the steps you are doing?\n> I assume we can get a better idea on where the EPERM comes from using\n> stap/ftrace.\n> \n> When attaching the snapshot to the backup VM, it looks like qemu is able to\n> open the first image of the chain, and the subsequent one fails:\n> \n> Could not open backing file: Could not open\n> '/rhev/data-center/58bdc405-028f-00dc-0026-0000000000aa/a3a4dfce-ca1b-4453-\n> a40b-dc43c4f2191a/images/1c28bd60-8556-40fd-8229-f52bdd51ca7a/25165986-adaa-\n> 4454-a7de-2d757b34e8f5': Operation not permitted"
									},
									{
										"isprivate": "0",
										"commentid": "11124883",
										"comment_count": "4",
										"who": {
											"text": "gveitmic",
											"name": "Germano Veit Michel"
										},
										"bug_when": "2017-12-20 04:59:12 +0000",
										"thetext": "(In reply to Raz Tamir from comment #2)\n> I'm not sure you are trying the same scenario mentioned in the original bug\n> What are the steps you are doing?\n\nI'm using the backup api script in a loop (similar to what Commvault and our customers do). But all the snapshot attachments work. What are you using? This?\n\n> On VM with 4 disks - A, B, C, D where A is the bootable disk with OS. After\n> starting the VM, if we will hot-unplug the 3 disks in order B, C and D and\n> after that will try to hotplug back disk C, it will fail with:"
									},
									{
										"isprivate": "0",
										"commentid": "11125239",
										"comment_count": "5",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2017-12-20 08:35:30 +0000",
										"thetext": "(In reply to Germano Veit Michel from comment #4)\n> (In reply to Raz Tamir from comment #2)\n> > I'm not sure you are trying the same scenario mentioned in the original bug\n> > What are the steps you are doing?\n> \n> I'm using the backup api script in a loop (similar to what Commvault and our\n> customers do). But all the snapshot attachments work. What are you using?\n> This?\n> \n> > On VM with 4 disks - A, B, C, D where A is the bootable disk with OS. After\n> > starting the VM, if we will hot-unplug the 3 disks in order B, C and D and\n> > after that will try to hotplug back disk C, it will fail with:\n\nYes,\n\nYou can try step 1 - 3 and only then, use the backup api method and attached the snapshot disk.  I think it will trigger the issue"
									},
									{
										"isprivate": "0",
										"commentid": "11128767",
										"comment_count": "6",
										"who": {
											"text": "gveitmic",
											"name": "Germano Veit Michel"
										},
										"bug_when": "2017-12-21 06:42:16 +0000",
										"thetext": "(In reply to Raz Tamir from comment #5)\n> Yes,\n> \n> You can try step 1 - 3 and only then, use the backup api method and attached\n> the snapshot disk.  I think it will trigger the issue\n\nThanks Raz, I was finally able to reproduce it.\n\nsystemd-219-42.el7_4.4.x86_64\nlibvirt-daemon-kvm-3.2.0-14.el7_4.5.x86_64\nvdsm-4.19.42-1.el7ev.x86_64\nkernel-3.10.0-693.11.1.el7.x86_64\nqemu-kvm-rhev-2.9.0-16.el7_4.11.x86_64\n\nIt seems only reproducible on Block Storage (on NFS I could not).\n\nThese are the conditions I found:\nVM with 4 disks, A, B, C, D\nBoot\nUnplug B, C an D\nAll hotplugs will fail unless I hotplug B. If B is plugged, the other 2 seem to work.\nMay not be 100% accurate, but it's what I found after playing with it.\n\nMoving forward looks like the problem appears for qemu with a open syscall returning EPERM, doesn't seem related to SELinux.\n\n# getenforce \nPermissive\n\n# strace -p 3521 -e trace=open\nstrace: Process 3521 attached\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDONLY|O_NONBLOCK|O_CLOEXEC) = -1 EPERM (Operation not permitted)\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDWR|O_DIRECT|O_CLOEXEC) = -1 EPERM (Operation not permitted)\n\nSo open returns -1. It's a bit weird that it's a EPERM (not ACCESS).\n\nThen I created an audit rule to track \"open\", capture both a success and a failure opening the same disk:\n# auditctl -a always,exit -F arch=b64 -F pid=3521 -S open -k qemu-open\n\nFAIL:\ntime->Thu Dec 21 14:30:27 2017\ntype=PROCTITLE msg=audit(1513830627.873:3248): proctitle=2F7573722F6C6962657865632F71656D752D6B766D002D6E616D650067756573743D54657374564D2C64656275672D746872656164733D6F6E002D53002D6F626A656374007365637265742C69643D6D61737465724B6579302C666F726D61743D7261772C66696C653D2F7661722F6C69622F6C6962766972742F71656D752F\ntype=PATH msg=audit(1513830627.873:3248): item=0 name=\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\" inode=53420 dev=00:27 mode=060660 ouid=36 ogid=107 rdev=fc:09 obj=system_u:object_r:svirt_image_t:s0:c83,c485 objtype=NORMAL\ntype=CWD msg=audit(1513830627.873:3248):  cwd=\"/\"\ntype=SYSCALL msg=audit(1513830627.873:3248): arch=c000003e syscall=2 success=no exit=-1 a0=5638be345340 a1=84002 a2=0 a3=7fd3d2803b00 items=1 ppid=1 pid=3521 auid=4294967295 uid=107 gid=107 euid=107 suid=107 fsuid=107 egid=107 sgid=107 fsgid=107 tty=(none) ses=4294967295 comm=\"qemu-kvm\" exe=\"/usr/libexec/qemu-kvm\" subj=system_u:system_r:svirt_t:s0:c83,c485 key=\"qemu-open\"\n\nSUCCESS:\ntime->Thu Dec 21 14:31:41 2017\ntype=PROCTITLE msg=audit(1513830701.500:3321): proctitle=2F7573722F6C6962657865632F71656D752D6B766D002D6E616D650067756573743D54657374564D2C64656275672D746872656164733D6F6E002D53002D6F626A656374007365637265742C69643D6D61737465724B6579302C666F726D61743D7261772C66696C653D2F7661722F6C69622F6C6962766972742F71656D752F\ntype=PATH msg=audit(1513830701.500:3321): item=0 name=\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\" inode=53420 dev=00:27 mode=060660 ouid=36 ogid=107 rdev=fc:09 obj=system_u:object_r:svirt_image_t:s0:c83,c485 objtype=NORMAL\ntype=CWD msg=audit(1513830701.500:3321):  cwd=\"/\"\ntype=SYSCALL msg=audit(1513830701.500:3321): arch=c000003e syscall=2 success=yes exit=77 a0=5638be346730 a1=80800 a2=0 a3=ffffff80 items=1 ppid=1 pid=3521 auid=4294967295 uid=107 gid=107 euid=107 suid=107 fsuid=107 egid=107 sgid=107 fsgid=107 tty=(none) ses=4294967295 comm=\"qemu-kvm\" exe=\"/usr/libexec/qemu-kvm\" subj=system_u:system_r:svirt_t:s0:c83,c485 key=\"qemu-open\"\n\nIt seems everything is exactly the same? One succeeds and one fails?\n\nI tried to stap to try to get some more info, but couldn't get far enough with my current stap, pretty much just confirmed the problem:\n\nprobe kernel.function(\"sys_open\").return {\n    if (pid() != 2090)\n        next\n    if ($return != -1)\n        next\n    printf(\"open returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))    \n    next\n}\n\nopen returned -1, performed by qemu-kvm, pid 2038\n  filename=0x55beedebdad0 flags=0x80800 mode=0x0\nopen returned -1, performed by qemu-kvm, pid 2038\n  filename=0x55beedebdad0 flags=0x84002 mode=0x0\n\nI would get to get a \"forward trace\" of all the functions sys_open called to get an idea where the -1 is coming from. But with a return probe, that is not going to happen.\nAnd ftrace is showing too much data (mostly selinux stuff), pretty much unreadable.\n\nAnd this is where I stopped today. Any ideas how to move forward? I think I'll try downgrading some packages tomorrow. Looking at the customer cases, this seem to have started with 7.4?"
									},
									{
										"isprivate": "0",
										"commentid": "11131188",
										"comment_count": "7",
										"who": {
											"text": "gveitmic",
											"name": "Germano Veit Michel"
										},
										"bug_when": "2017-12-22 00:37:29 +0000",
										"thetext": "Tracked it down to cgroups within the kernel:\n\n# cat qemu_open.stp \nprobe kernel.function(\"do_sys_open\").return {\n    if (pid() != 2632) \n        next\n    if ($return != -1)\n        next\n    printf(\"do_sys_open returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))\n}\n\nprobe kernel.function(\"__devcgroup_check_permission\").return {\n    if (pid() != 2632) \n        next\n    if ($return != -1)\n        next\n    printf(\"__devcgroup_check_permission returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))\n}\n\nDuring failure, we have this:\n\n# stap qemu_open.stp \n__devcgroup_check_permission returned -1, performed by qemu-kvm, pid 2632\n  type=0x1 major=0xfc minor=0x9 access=0x2\ndo_sys_open returned -1, performed by qemu-kvm, pid 2632\n  dfd=0xffffffffffffff9c filename=0x56354c7dfa20 flags=0x88800 mode=0x0\n__devcgroup_check_permission returned -1, performed by qemu-kvm, pid 2632\n  type=0x1 major=0xfc minor=0x9 access=0x6\ndo_sys_open returned -1, performed by qemu-kvm, pid 2632\n  dfd=0xffffffffffffff9c filename=0x56354c7dfa20 flags=0x8c002 mode=0x0\n\nSo __devcgroup_check_permission generates the EPERM, which propagates all the way back to sys_do_open, and qemu-kvm gets this:\n\n# strace -p 2632 -e trace=open\nstrace: Process 2632 attached\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDONLY|O_NONBLOCK|O_CLOEXEC) = -1 EPERM (Operation not permitted)\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDWR|O_DIRECT|O_CLOEXEC) = -1 EPERM (Operation not permitted)\n\nHere an idea on what happens within the kernel at that point:\n\n\nqemu-kvm-2632  [000]   816.223260: funcgraph_entry:            |            may_open() {\nqemu-kvm-2632  [000]   816.223260: funcgraph_entry:            |              inode_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry:            |                __inode_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry:            |                  generic_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry: 0.038 us   |                    get_acl();\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry: 0.043 us   |                    in_group_p();\nqemu-kvm-2632  [000]   816.223261: funcgraph_exit:  0.557 us   |                  }\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry:            |                  __devcgroup_inode_permission() {\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry:            |                    __devcgroup_check_permission() {\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry: 0.317 us   |                      match_exception();\nqemu-kvm-2632  [000]   816.223262: funcgraph_exit:  0.597 us   |                    }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  0.856 us   |                  }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.018 us   |                }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.339 us   |              }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.636 us   |            }\n\nqemu-kvm-2632  [000]   816.223272: sys_exit_open:        0xffffffffffffffff\n\nSo, looks like a libvirt bug with cgroups?\n\nI'm going on PTO for 2 weeks and wont be able to continue this. But I assume the next step would be to track cgroup manipulation within libvirt or involve libvirt devel."
									},
									{
										"isprivate": "0",
										"commentid": "11136377",
										"comment_count": "9",
										"who": {
											"text": "ehabkost",
											"name": "Eduardo Habkost"
										},
										"bug_when": "2017-12-26 18:16:53 +0000",
										"thetext": "Is there a sosreport from a host after the failure is reproduced, somewhere? I would like to take a look at the croup data."
									},
									{
										"isprivate": "0",
										"commentid": "11136579",
										"comment_count": "11",
										"who": {
											"text": "ehabkost",
											"name": "Eduardo Habkost"
										},
										"bug_when": "2017-12-26 23:02:37 +0000",
										"thetext": "I just found out that the audit logs are broken due to a libvirt bug. See https://github.com/ehabkost/libvirt/commit/fa7b97da69595ec4b8992ccaacfe5a7347436d6a"
									},
									{
										"isprivate": "0",
										"commentid": "11137699",
										"comment_count": "12",
										"who": {
											"text": "ehabkost",
											"name": "Eduardo Habkost"
										},
										"bug_when": "2017-12-27 18:00:10 +0000",
										"thetext": "I need help from somebody familiar with VDSM to reproduce the bug. Is there anybody available this week who is able to reproduce the bug and can give me access to the host where it can be seen?"
									},
									{
										"isprivate": "0",
										"commentid": "11138751",
										"comment_count": "14",
										"who": {
											"text": "ehabkost",
											"name": "Eduardo Habkost"
										},
										"bug_when": "2017-12-28 11:25:25 +0000",
										"thetext": "Good news: I think I have just reproduced it using libvirt\ndirectly, after activating/deactivating the LVM volumes on every\nattach/detach operation.  I think the bug is caused by block\ndevice major/minor number reuse when the devices are re-enabled\nin a different order.\n\nI will investigate further and report back soon."
									},
									{
										"isprivate": "0",
										"commentid": "11138995",
										"comment_count": "15",
										"who": {
											"text": "ehabkost",
											"name": "Eduardo Habkost"
										},
										"bug_when": "2017-12-28 14:30:57 +0000",
										"thetext": "I confirm that this a bug on libvirt's namespace handling.  It's possible to work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf."
									},
									{
										"isprivate": "0",
										"commentid": "11139317",
										"comment_count": "16",
										"who": {
											"text": "ehabkost",
											"name": "Eduardo Habkost"
										},
										"bug_when": "2017-12-28 18:30:53 +0000",
										"thetext": "Experimental fix at:\nhttps://github.com/ehabkost/libvirt/commit/89f1a08b9518148f6a86600c0ded6f52886e44b4"
									},
									{
										"isprivate": "0",
										"commentid": "11143389",
										"comment_count": "17",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2017-12-31 09:44:18 +0000",
										"thetext": "(In reply to Eduardo Habkost from comment #15)\n> I confirm that this a bug on libvirt's namespace handling.  It's possible to\n> work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf.\n\nThe suggested W/A worked for me:\n\n1) added namespaces = [ ] to /etc/libvirt/qemu.conf\n2) # systemctl restart vdsmd\n3) # systemctl restart libvirtd\n4) and performed the steps to reproduce from the original bug."
									},
									{
										"isprivate": "0",
										"commentid": "11205123",
										"comment_count": "18",
										"who": {
											"text": "amureini",
											"name": "Allon Mureinik"
										},
										"bug_when": "2018-01-22 13:26:53 +0000",
										"thetext": "A proper fix depends on a libvirt fix that is not yet released. Pushing out to 4.1.10."
									},
									{
										"isprivate": "0",
										"commentid": "11315339",
										"comment_count": "22",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2018-02-23 13:12:39 +0000",
										"thetext": "This bug is no longer depends on platform side as all depended bug was addressed.\n\nDaniel,\n\nCan you take a look at this?"
									},
									{
										"isprivate": "0",
										"commentid": "11318663",
										"comment_count": "23",
										"who": {
											"text": "amureini",
											"name": "Allon Mureinik"
										},
										"bug_when": "2018-02-25 11:02:41 +0000",
										"thetext": "(In reply to Raz Tamir from comment #22)\n> This bug is no longer depends on platform side as all depended bug was\n> addressed.\n> \n> Daniel,\n> \n> Can you take a look at this?\n\nThe libvirt build containing the fix was not released yet.\nRaz, did you test with an unreleased libvirt referenced in the platform bug? Could you clarify the above statement?"
									},
									{
										"isprivate": "0",
										"commentid": "11318879",
										"comment_count": "24",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2018-02-25 16:33:17 +0000",
										"thetext": "(In reply to Allon Mureinik from comment #23)\n> (In reply to Raz Tamir from comment #22)\n> > This bug is no longer depends on platform side as all depended bug was\n> > addressed.\n> > \n> > Daniel,\n> > \n> > Can you take a look at this?\n> \n> The libvirt build containing the fix was not released yet.\n> Raz, did you test with an unreleased libvirt referenced in the platform bug?\n> Could you clarify the above statement?\n\nHi Allon,\n\nBoth 'depends on' bugs are 'VERIFIED'.\nWe are currently working with the latest snapshot from platform before GA.\nWhen should we get it?"
									},
									{
										"isprivate": "0",
										"commentid": "11319871",
										"comment_count": "25",
										"who": {
											"text": "amureini",
											"name": "Allon Mureinik"
										},
										"bug_when": "2018-02-26 08:01:56 +0000",
										"thetext": "(In reply to Raz Tamir from comment #24)\n> (In reply to Allon Mureinik from comment #23)\n> > (In reply to Raz Tamir from comment #22)\n> > > This bug is no longer depends on platform side as all depended bug was\n> > > addressed.\n> > > \n> > > Daniel,\n> > > \n> > > Can you take a look at this?\n> > \n> > The libvirt build containing the fix was not released yet.\n> > Raz, did you test with an unreleased libvirt referenced in the platform bug?\n> > Could you clarify the above statement?\n> \n> Hi Allon,\n> \n> Both 'depends on' bugs are 'VERIFIED'.\n> We are currently working with the latest snapshot from platform before GA.\n> When should we get it?\n\nI think we may be muddling two different things here.\n\nFrom a QE perspective - testing with pre-release libvirt bits is the right way to go, IMHO. If the newer libvirt solves the issue - great; if not - the problem should be escalated to libvirt's devs.\n\nFrom a RHV dev perspective - there's no AI on RHV's code. The only AI we have is to require the newer libvirt version that contains the fix in vdsm's spec file (and even that is just a nicety - you can always upgrade libvirt yourself and gain the benefits of the newer version regardless of vdsm's requirements). We cannot update vdsm's spec to require a libvirt version that isn't available yet, as this will break the ability to install vdsm.\n\nAccording to libvirt's 7.4.z bug, bz#1532183, their fix will be delivered by erratum RHBA-2018:32240-04. It is currently scheduled for March 6th."
									},
									{
										"isprivate": "0",
										"commentid": "11320137",
										"comment_count": "26",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2018-02-26 09:21:24 +0000",
										"thetext": "Thanks for clarification Allon"
									},
									{
										"isprivate": "0",
										"commentid": "11384339",
										"comment_count": "31",
										"who": {
											"text": "ratamir",
											"name": "Raz Tamir"
										},
										"bug_when": "2018-03-15 16:13:57 +0000",
										"thetext": "Verified on rhvm-4.2.2.2-0.1.el7\n\nFollowed steps to reproduce - Pass"
									},
									{
										"isprivate": "0",
										"commentid": "11600581",
										"comment_count": "36",
										"who": {
											"text": "errata-xmlrpc",
											"name": "errata-xmlrpc"
										},
										"bug_when": "2018-05-15 17:54:02 +0000",
										"thetext": "Since the problem described in this bug report should be\nresolved in a recent advisory, it has been closed with a\nresolution of ERRATA.\n\nFor information on the advisory, and where to find the updated\nfiles, follow the link below.\n\nIf the solution does not work for you, open a new bug report.\n\nhttps://access.redhat.com/errata/RHEA-2018:1489"
									},
									{
										"isprivate": "0",
										"commentid": "12743284",
										"comment_count": "37",
										"who": {
											"text": "fkust",
											"name": "Franta Kust"
										},
										"bug_when": "2019-05-16 13:04:43 +0000",
										"thetext": "BZ<2>Jira Resync"
									},
									{
										"isprivate": "0",
										"commentid": "13114488",
										"comment_count": "38",
										"who": {
											"text": "dagur",
											"name": "Daniel Gur"
										},
										"bug_when": "2019-08-28 13:12:20 +0000",
										"thetext": "sync2jira"
									},
									{
										"isprivate": "0",
										"commentid": "13114987",
										"comment_count": "39",
										"who": {
											"text": "dagur",
											"name": "Daniel Gur"
										},
										"bug_when": "2019-08-28 13:16:32 +0000",
										"thetext": "sync2jira"
									}
								]
							}
						],
						"external_bugs": {
							"text": "87630",
							"name": "oVirt gerrit"
						},
						"long_desc": [
							{
								"isprivate": "0",
								"commentid": "10579954",
								"comment_count": "0",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-07-10 17:00:45 +0000",
								"thetext": "Description of problem:\nOn VM with 4 disks - A, B, C, D where A is the bootable disk with OS. After starting the VM, if we will hot-unplug the 3 disks in order B, C and D and after that will try to hotplug back disk C, it will fail with:\nVDSM host_mixed_2 command HotPlugDiskVDS failed: Internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be initialized\n\nengine.log:\n\n2017-07-10 19:47:12,806+03 INFO  [org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] Running command: HotPlugDiskToVmCommand in\nternal: false. Entities affected :  ID: 848657b3-1dc1-4c12-873c-b819e3415840 Type: VMAction group CONFIGURE_VM_STORAGE with role type USER\n2017-07-10 19:47:12,819+03 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] START, HotPlugDiskVDSCommand(HostName = \nhost_mixed_2, HotPlugDiskVDSParameters:{runAsync='true', hostId='ce31742f-1f79-4d5a-84c8-103e194029e2', vmId='848657b3-1dc1-4c12-873c-b819e3415840', diskId='573314a3-45d9-49a2-80eb-9d62c8bbdf2c', addressMap='null'}\n), log id: 69315296\n2017-07-10 19:47:13,858+03 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.FullListVDSCommand] (DefaultQuartzScheduler4) [25f64cfb] START, FullListVDSCommand(HostName = host_mixed_2, FullListVDSCommandParameters:{\nrunAsync='true', hostId='ce31742f-1f79-4d5a-84c8-103e194029e2', vmIds='[848657b3-1dc1-4c12-873c-b819e3415840]'}), log id: 6bc6800d\n2017-07-10 19:47:14,205+03 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.FullListVDSCommand] (DefaultQuartzScheduler4) [25f64cfb] FINISH, FullListVDSCommand, return: [{acpiEnable=true, emulatedMachine=pc-i440fx-\nrhel7.3.0, vmId=848657b3-1dc1-4c12-873c-b819e3415840, guestDiskMapping={bf334992-dd70-48dd-8={name=/dev/vdb}, b19764da-eb8f-4c41-a={name=/dev/vda}, 573314a3-45d9-49a2-8={name=/dev/vdc}, QEMU_DVD-ROM_QM00003={name=/\ndev/sr0}, 0QEMU_QEMU_HARDDISK_1ed10d71-7cc5-42aa-b={name=/dev/sda}}, transparentHugePages=true, timeOffset=0, cpuType=Conroe, smp=1, pauseCode=NOERR, guestNumaNodes=[Ljava.lang.Object;@2a124cc2, smartcardEnable=fal\nse, custom={device_1a6f0061-74c1-4c9d-a7d8-f6500b6cc79b=VmDevice:{id='VmDeviceId:{deviceId='1a6f0061-74c1-4c9d-a7d8-f6500b6cc79b', vmId='848657b3-1dc1-4c12-873c-b819e3415840'}', device='ide', type='CONTROLLER', boo\ntOrder='0', specParams='[]', address='{slot=0x01, bus=0x00, domain=0x0000, type=pci, function=0x1}', managed='false', plugged='true', readOnly='false', deviceAlias='ide', customProperties='[]', snapshotId='null', l\nogicalName='null', hostDevice='null'}, device_1a6f0061-74c1-4c9d-a7d8-f6500b6cc79bdevice_65c98fdd-f9be-48ea-92af-3658a0938762=VmDevice:{id='VmDeviceId:{deviceId='65c98fdd-f9be-48ea-92af-3658a0938762', vmId='848657b\n3-1dc1-4c12-873c-b819e3415840'}', device='unix', type='CHANNEL', bootOrder='0', specParams='[]', address='{bus=0, controller=0, type=virtio-serial, port=1}', managed='false', plugged='true', readOnly='false', devic\neAlias='channel0', customProperties='[]', snapshotId='null', logicalName='null', hostDevice='null'}, device_1a6f0061-74c1-4c9d-a7d8-f6500b6cc79bdevice_65c98fdd-f9be-48ea-92af-3658a0938762device_6bcf3112-5ed9-4d38-a\ncc4-1992ecbfc49adevice_03d1e32c-cd35-4887-9c5c-8e6196e3d65c=VmDevice:{id='VmDeviceId:{deviceId='03d1e32c-cd35-4887-9c5c-8e6196e3d65c', vmId='848657b3-1dc1-4c12-873c-b819e3415840'}', device='spicevmc', type='CHANNEL\n', bootOrder='0', specParams='[]', address='{bus=0, controller=0, type=virtio-serial, port=3}', managed='false', plugged='true', readOnly='false', deviceAlias='channel2', customProperties='[]', snapshotId='null', l\nogicalName='null', hostDevice='null'}, device_1a6f0061-74c1-4c9d-a7d8-f6500b6cc79bdevice_65c98fdd-f9be-48ea-92af-3658a0938762device_6bcf3112-5ed9-4d38-acc4-1992ecbfc49a=VmDevice:{id='VmDeviceId:{deviceId='6bcf3112-\n5ed9-4d38-acc4-1992ecbfc49a', vmId='848657b3-1dc1-4c12-873c-b819e3415840'}', device='unix', type='CHANNEL', bootOrder='0', specParams='[]', address='{bus=0, controller=0, type=virtio-serial, port=2}', managed='fals\ne', plugged='true', readOnly='false', deviceAlias='channel1', customProperties='[]', snapshotId='null', logicalName='null', hostDevice='null'}}, vmType=kvm, memSize=1024, smpCoresPerSocket=1, vmName=vm_iscsi_1_iscs\ni_1018421468, nice=0, status=Up, maxMemSize=4096, bootMenuEnable=false, pid=32744, smpThreadsPerCore=1, memGuaranteedSize=1024, kvmEnable=true, pitReinjection=false, displayNetwork=ovirtmgmt, devices=[Ljava.lang.Ob\nject;@12735a0c, display=qxl, maxVCpus=16, clientIp=10.35.4.157, statusTime=6031012890, maxMemSlots=16}], log id: 6bc6800d\n2017-07-10 19:47:14,224+03 INFO  [org.ovirt.engine.core.vdsbroker.monitoring.VmDevicesMonitoring] (DefaultQuartzScheduler4) [25f64cfb] Received a spice Device without an address when processing VM 848657b3-1dc1-4c1\n2-873c-b819e3415840 devices, skipping device: {device=spice, specParams={fileTransferEnable=true, displayNetwork=ovirtmgmt, copyPasteEnable=true, displayIp=10.35.82.68, spiceSecureChannels=smain,sinputs,scursor,spl\nayback,srecord,sdisplay,ssmartcard,susbredir}, type=graphics, deviceId=f501cb72-48ef-481a-b689-530ebf1bc483, tlsPort=5900}\n2017-07-10 19:47:15,381+03 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] Failed in 'HotPlugDiskVDS' method\n2017-07-10 19:47:15,395+03 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] EVENT_ID: VDS_BROKER_COMMAND_FAILUR\nE(10,802), Correlation ID: null, Call Stack: null, Custom Event ID: -1, Message: VDSM host_mixed_2 command HotPlugDiskVDS failed: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'dri\nve-virtio-disk3' could not be initialized\n2017-07-10 19:47:15,395+03 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] Command 'org.ovirt.engine.core.vdsbroker\n.vdsbroker.HotPlugDiskVDSCommand' return value 'StatusOnlyReturn [status=Status [code=45, message=internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be in\nitialized]]'\n2017-07-10 19:47:15,395+03 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] HostName = host_mixed_2\n2017-07-10 19:47:15,396+03 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] Command 'HotPlugDiskVDSCommand(HostName \n= host_mixed_2, HotPlugDiskVDSParameters:{runAsync='true', hostId='ce31742f-1f79-4d5a-84c8-103e194029e2', vmId='848657b3-1dc1-4c12-873c-b819e3415840', diskId='573314a3-45d9-49a2-80eb-9d62c8bbdf2c', addressMap='null\n'})' execution failed: VDSGenericException: VDSErrorException: Failed to HotPlugDiskVDS, error = internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be ini\ntialized, code = 45\n2017-07-10 19:47:15,396+03 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] FINISH, HotPlugDiskVDSCommand, log id: 6\n9315296\n2017-07-10 19:47:15,396+03 ERROR [org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] Command 'org.ovirt.engine.core.bll.storage\n.disk.HotPlugDiskToVmCommand' failed: EngineException: org.ovirt.engine.core.vdsbroker.vdsbroker.VDSErrorException: VDSGenericException: VDSErrorException: Failed to HotPlugDiskVDS, error = internal error: unable t\no execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be initialized, code = 45 (Failed with error FailedToPlugDisk and code 45)\n2017-07-10 19:47:15,409+03 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (org.ovirt.thread.pool-7-thread-38) [9185722b-577f-4462-86a8-e3a264f92d8e] EVENT_ID: USER_FAILED_HOTPLUG_DISK(2,001), Correlation ID: 9185722b-577f-4462-86a8-e3a264f92d8e, Call Stack: null, Custom Event ID: -1, Message: Failed to plug disk disk_to_plug_iscsi_6 to VM vm_iscsi_1_iscsi_1018421468 (User: admin@internal-authz).\n\nvdsm.log:\n\n2017-07-10 19:47:13,687+0300 INFO  (jsonrpc/7) [vds] prepared volume path: /rhev/data-center/290170d6-2703-47a4-acd1-252655cba202/10868c43-fea0-4c2a-8da7-3b21d7c80e68/images/573314a3-45d9-49a2-80eb-9d62c8bbdf2c/3f8d5490-6517-4543-94e0-dc98b3cd12fe (clientIF:374)\n2017-07-10 19:47:13,689+0300 INFO  (jsonrpc/7) [vdsm.api] START getVolumeSize(sdUUID=u'10868c43-fea0-4c2a-8da7-3b21d7c80e68', spUUID=u'290170d6-2703-47a4-acd1-252655cba202', imgUUID=u'573314a3-45d9-49a2-80eb-9d62c8bbdf2c', volUUID=u'3f8d5490-6517-4543-94e0-dc98b3cd12fe', options=None) from=::ffff:10.35.161.131,49826, flow_id=9185722b-577f-4462-86a8-e3a264f92d8e, task_id=b982d92c-5097-48bc-bf6b-dd41d0d1d323 (api:46)\n2017-07-10 19:47:13,690+0300 INFO  (jsonrpc/7) [vdsm.api] FINISH getVolumeSize return={'truesize': '1073741824', 'apparentsize': '1073741824'} from=::ffff:10.35.161.131,49826, flow_id=9185722b-577f-4462-86a8-e3a264f92d8e, task_id=b982d92c-5097-48bc-bf6b-dd41d0d1d323 (api:52)\n2017-07-10 19:47:13,701+0300 INFO  (jsonrpc/7) [virt.vm] (vmId='848657b3-1dc1-4c12-873c-b819e3415840') Hotplug disk xml: <?xml version='1.0' encoding='UTF-8'?>\n<disk address=\"\" device=\"disk\" snapshot=\"no\" type=\"block\">\n    <source dev=\"/rhev/data-center/290170d6-2703-47a4-acd1-252655cba202/10868c43-fea0-4c2a-8da7-3b21d7c80e68/images/573314a3-45d9-49a2-80eb-9d62c8bbdf2c/3f8d5490-6517-4543-94e0-dc98b3cd12fe\" />\n    <target bus=\"virtio\" dev=\"vdd\" />\n    <serial>573314a3-45d9-49a2-80eb-9d62c8bbdf2c</serial>\n    <driver cache=\"none\" error_policy=\"stop\" io=\"native\" name=\"qemu\" type=\"qcow2\" />\n</disk>\n (vm:2996)\n2017-07-10 19:47:13,752+0300 ERROR (jsonrpc/7) [virt.vm] (vmId='848657b3-1dc1-4c12-873c-b819e3415840') Hotplug failed (vm:3004)\nTraceback (most recent call last):\n  File \"/usr/share/vdsm/virt/vm.py\", line 3002, in hotplugDisk\n    self._dom.attachDevice(driveXml)\n  File \"/usr/lib/python2.7/site-packages/vdsm/virt/virdomain.py\", line 69, in f\n    ret = attr(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/libvirtconnection.py\", line 123, in wrapper\n    ret = f(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/utils.py\", line 941, in wrapper\n    return func(inst, *args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 540, in attachDevice\n    if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\nlibvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be initialized\n\n\n\nVersion-Release number of selected component (if applicable):\nrhevm-4.1.4-0.2.el7.noarch\nvdsm-4.19.21-1.el7ev.x86_64\nlibvirt-client-3.2.0-14.el7.x86_64\nqemu-kvm-rhev-2.9.0-16.el7.x86_64\n\n\nHow reproducible:\n100%\n\nSteps to Reproduce:\n1. Create a VM with 1 disk (A) + OS\n2. Attach new 3 disks (B, C and D) - active\n3. Start the VM - wait for the OS to finish the boot sequence\n3. Hot unplug the disks in that order - B -> C -> D\n4. Hotplug disk C again\n\nActual results:\nVDSM host_mixed_2 command HotPlugDiskVDS failed: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be initialized\n\nExpected results:\n\n\nAdditional info:"
							},
							{
								"isprivate": "0",
								"commentid": "10581679",
								"comment_count": "1",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2017-07-11 08:46:37 +0000",
								"thetext": "@Raz - can you please attach full logs, including libvirt and qemu. Also, which OS did you use?"
							},
							{
								"isprivate": "0",
								"commentid": "10581780",
								"comment_count": "2",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-07-11 09:14:16 +0000",
								"thetext": "Created attachment 1296139\nengine and vdsm logs\n\nHi Daniel,\n\nFull logs attached, libvirt doesn't exist anymore.\n\nI used el 7.3 but it also happens on 7.4"
							},
							{
								"isprivate": "0",
								"commentid": "10581783",
								"comment_count": "3",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-07-11 09:14:36 +0000",
								"thetext": "Created attachment 1296140\nqemu"
							},
							{
								"isprivate": "0",
								"commentid": "10582639",
								"comment_count": "4",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2017-07-11 13:20:11 +0000",
								"thetext": "According to qemu log[1], it seems there's some permission issue. Can you please reproduce the scenario again and attach libvirt log, so we could ask libvirt dev for advise on the libvirtError[2].\n\n[1] Could not open '/rhev/data-center/290170d6-2703-47a4-acd1-252655cba202/10868c43-fea0-4c2a-8da7-3b21d7c80e68/images/573314a3-45d9-49a2-80eb-9d62c8bbdf2c/3f8d5490-6517-4543-94e0-dc98b3cd12fe': Operation not permitted\n\n[2] libvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be initialized"
							},
							{
								"isprivate": "0",
								"commentid": "10586102",
								"comment_count": "5",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-07-12 07:39:50 +0000",
								"thetext": "Hi Daniel,\n\nIs there any issue to reproduce this on your environment?\nLet me know if you need my help"
							},
							{
								"isprivate": "0",
								"commentid": "10600742",
								"comment_count": "6",
								"who": {
									"text": "amureini",
									"name": "Allon Mureinik"
								},
								"bug_when": "2017-07-17 13:38:13 +0000",
								"thetext": "Also, to help triage the problem - can we retry this on RHEL7.3 please?"
							},
							{
								"isprivate": "0",
								"commentid": "10604806",
								"comment_count": "7",
								"who": {
									"text": "amureini",
									"name": "Allon Mureinik"
								},
								"bug_when": "2017-07-18 12:33:34 +0000",
								"thetext": "(In reply to Allon Mureinik from comment #6)\n> Also, to help triage the problem - can we retry this on RHEL7.3 please?\nMissed comment 2, my apologies."
							},
							{
								"isprivate": "0",
								"commentid": "10623306",
								"comment_count": "8",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2017-07-24 16:19:21 +0000",
								"thetext": "(In reply to Raz Tamir from comment #5)\n> Hi Daniel,\n> \n> Is there any issue to reproduce this on your environment?\n> Let me know if you need my help\n\nI've couldn't reproduce it on el7.3 nor el7.4. Was it reproduced in any other environment? Reproduced in a specific configuration (block/file/disk interface)?"
							},
							{
								"isprivate": "0",
								"commentid": "10625746",
								"comment_count": "9",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-07-25 09:16:35 +0000",
								"thetext": "Reproduced in my environment.\nDaniel, you can fetch all logs needed from my environment"
							},
							{
								"isprivate": "0",
								"commentid": "10675775",
								"comment_count": "10",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2017-08-06 14:01:57 +0000",
								"thetext": "IIUC, seems that a similar issue has been encountered in specific versions of qemu-img/qemu-kvm:\n\nhttps://ask.openstack.org/en/question/56961/returning-exception-internal-error-unable-to-execute-qemu-command-__comredhat_drive_add-device-drive-virtio-disk1-could-not-be-initialized-to-caller/\n\n@Kevin - is it a known issue?"
							},
							{
								"isprivate": "0",
								"commentid": "10677197",
								"comment_count": "11",
								"who": {
									"text": "kwolf",
									"name": "Kevin Wolf"
								},
								"bug_when": "2017-08-07 09:19:43 +0000",
								"thetext": "I am not aware of the issue. It looks very much like a problem outside of qemu,\nbecause the kernel seems to return -EPERM for the open() syscall. You can confirm\nthis with strace.\n\nI seem to remember that we had such problems related to libvirt not applying the\ncorrect SELinux labels to the image, so SELinux is where I'd look first."
							},
							{
								"isprivate": "0",
								"commentid": "10981624",
								"comment_count": "20",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2017-11-13 13:43:05 +0000",
								"thetext": "(In reply to Kevin Wolf from comment #11)\n> I am not aware of the issue. It looks very much like a problem outside of\n> qemu,\n> because the kernel seems to return -EPERM for the open() syscall. You can\n> confirm\n> this with strace.\n> \n> I seem to remember that we had such problems related to libvirt not applying\n> the\n> correct SELinux labels to the image, so SELinux is where I'd look first.\n\n@Eric - is there any known issue with libvirt applying SELinux labels?\nSeems we're sporadically getting a libvirtError on disk attach [1].\nSee also event failure in comment 18 [2].\n\n[1]\n  File \"/usr/share/vdsm/virt/vm.py\", line 3002, in hotplugDisk\n    self._dom.attachDevice(driveXml)\n  File \"/usr/lib/python2.7/site-packages/vdsm/virt/virdomain.py\", line 69, in f\n    ret = attr(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/libvirtconnection.py\", line 123, in wrapper\n    ret = f(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/utils.py\", line 941, in wrapper\n    return func(inst, *args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 540, in attachDevice\n    if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\nlibvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk3' could not be initialized\n\n[2]\ntype=VIRT_RESOURCE msg=audit(1510241246.816:4418878): pid=3405 uid=0 auid=4294967295 ses=4294967295 subj=system_u:system_r:virtd_t:s0-s0:c0.c1023 msg='virt=kvm resrc=disk reason=attach vm=\"backup-rhv\" uuid=1bfe3e00-0b33-41f6-b413-dc64b9eda5b8 old-disk=\"?\" new-disk=\"/var/lib/vdsm/transient/ac8a04d6-bdcf-4516-8956-21f495a8ffda-a1539e2d-cc80-4c48-becf-ba81d86f012f.8Sj5f9\" exe=\"/usr/sbin/libvirtd\" hostname=? addr=? terminal=? res=failed'"
							},
							{
								"isprivate": "0",
								"commentid": "10982410",
								"comment_count": "21",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-11-13 15:40:48 +0000",
								"thetext": "Raz, can you reproduce this with selinux permissive mode?\n\nIf you can, please attach the output of:\n\n    ausearch -m AVC"
							},
							{
								"isprivate": "0",
								"commentid": "10982413",
								"comment_count": "22",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-11-13 15:41:46 +0000",
								"thetext": "Jason, provide the info requested in comment 21?"
							},
							{
								"isprivate": "0",
								"commentid": "10982491",
								"comment_count": "23",
								"who": {
									"text": "eblake",
									"name": "Eric Blake"
								},
								"bug_when": "2017-11-13 15:56:25 +0000",
								"thetext": "Widening the query in comment #20 to libvirt-maint"
							},
							{
								"isprivate": "0",
								"commentid": "10986533",
								"comment_count": "25",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-11-14 14:02:24 +0000",
								"thetext": "Nir,\n\nMy input still relevant here?\nI see comment #24 also provided that info"
							},
							{
								"isprivate": "0",
								"commentid": "11038874",
								"comment_count": "26",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-11-28 09:09:34 +0000",
								"thetext": "(In reply to Raz Tamir from comment #25)\n> My input still relevant here?\n> I see comment #24 also provided that info\n\nYes, I want to know if you can reproduce in permissive mode."
							},
							{
								"isprivate": "0",
								"commentid": "11039586",
								"comment_count": "27",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2017-11-28 12:03:52 +0000",
								"thetext": "(In reply to Nir Soffer from comment #26)\n> (In reply to Raz Tamir from comment #25)\n> > My input still relevant here?\n> > I see comment #24 also provided that info\n> \n> Yes, I want to know if you can reproduce in permissive mode.\n\nReturning the needinfo."
							},
							{
								"isprivate": "0",
								"commentid": "11039849",
								"comment_count": "28",
								"who": {
									"text": "jsuchane",
									"name": "Jaroslav Suchanek"
								},
								"bug_when": "2017-11-28 13:04:39 +0000",
								"thetext": "Might be fixed by this:\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1506072\n\nPeter, can you have a look please? Thanks."
							},
							{
								"isprivate": "0",
								"commentid": "11080140",
								"comment_count": "30",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-12-08 11:50:45 +0000",
								"thetext": "(In reply to Nir Soffer from comment #21)\n> Raz, can you reproduce this with selinux permissive mode?\n> \n> If you can, please attach the output of:\n> \n>     ausearch -m AVC\n\nNir,\n\nI've set Permissive mode on the vdsms and reproduced the bug.\nHowever, executing 'ausearch -m AVC' returned no matches:\n[root@storage-ge5-vdsm3 ~]# ausearch -m AVC\n<no matches>\n\nHow to proceed from this point?"
							},
							{
								"isprivate": "0",
								"commentid": "11080305",
								"comment_count": "31",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-12-08 12:52:33 +0000",
								"thetext": "Maybe this is a duplicate of bug 1506157?\n\nRaz, can you test again with current vdsm master, or latest vdsm 4.1 (4.19.42)?\n\nWe require now libvirt-daemon >= 3.2.0-14.el7_4.5 - if this is a duplicate, it \nshould be fixed now.\n\nIf this still happens, please return the needinfo for Peter removed in comment 30\n(by mistake?)"
							},
							{
								"isprivate": "0",
								"commentid": "11080551",
								"comment_count": "32",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-12-08 14:30:41 +0000",
								"thetext": "Issue reproduced on:\nrhvm-4.2.0-0.6.el7\nvdsm-4.20.9-1.el7ev.x86_64\nlibvirt-3.2.0-14.el7_4.4.x86_64\n\nvdsm.log:\n2017-12-08 16:19:25,500+0200 ERROR (jsonrpc/5) [virt.vm] (vmId='629c80df-8c85-4c43-90de-319540132829') Hotplug failed (vm:3632)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/vdsm/virt/vm.py\", line 3630, in hotplugDisk\n    self._dom.attachDevice(driveXml)\n  File \"/usr/lib/python2.7/site-packages/vdsm/virt/virdomain.py\", line 98, in f\n    ret = attr(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/libvirtconnection.py\", line 126, in wrapper\n    ret = f(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/utils.py\", line 512, in wrapper\n    return func(inst, *args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 540, in attachDevice\n    if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\nlibvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized\n\nengine.log:\n2017-12-08 16:19:26,554+02 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] Failed in 'HotPlugDiskVDS' method\n2017-12-08 16:19:26,566+02 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] EVENT_ID: VDS_BROKER_COMMAND_FAILURE(10,802), VDSM host_mixed_1 command HotPlugDiskVDS failed: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized\n2017-12-08 16:19:26,566+02 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] Command 'org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand' return value 'StatusOnlyReturn [status=Status [code=45, message=internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized]]'\n2017-12-08 16:19:26,567+02 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] HostName = host_mixed_1\n2017-12-08 16:19:26,567+02 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] Command 'HotPlugDiskVDSCommand(HostName = host_mixed_1, HotPlugDiskVDSParameters:{hostId='20a94a01-73e4-431f-a796-cb2238512967', vmId='629c80df-8c85-4c43-90de-319540132829', diskId='8513de32-c652-48ee-b47f-01719cf39e43', addressMap='[bus=0, controller=0, unit=3, type=drive, target=0]'})' execution failed: VDSGenericException: VDSErrorException: Failed to HotPlugDiskVDS, error = internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized, code = 45\n2017-12-08 16:19:26,567+02 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] FINISH, HotPlugDiskVDSCommand, log id: 3e7236ec\n2017-12-08 16:19:26,568+02 ERROR [org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] Command 'org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand' failed: EngineException: org.ovirt.engine.core.vdsbroker.vdsbroker.VDSErrorException: VDSGenericException: VDSErrorException: Failed to HotPlugDiskVDS, error = internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized, code = 45 (Failed with error FailedToPlugDisk and code 45)\n2017-12-08 16:19:26,586+02 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (EE-ManagedThreadFactory-engine-Thread-39464) [a3fbe6ef-96a1-4d9a-aeb1-0f21a21c8a17] EVENT_ID: USER_FAILED_HOTPLUG_DISK(2,001), Failed to plug disk test_Disk2 to VM test (User: admin@internal-authz).\n\n\n\nI don't see libvirt-3.2.0-14.el7_4.5 available even on u/s 4.2\n\nre-adding the needinfo according comment #31"
							},
							{
								"isprivate": "0",
								"commentid": "11107850",
								"comment_count": "34",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-12-14 09:48:29 +0000",
								"thetext": "(In reply to Nir Soffer from comment #31)\n> Maybe this is a duplicate of bug 1506157?\n> \n> Raz, can you test again with current vdsm master, or latest vdsm 4.1\n> (4.19.42)?\n> \n> We require now libvirt-daemon >= 3.2.0-14.el7_4.5 - if this is a duplicate,\n> it \n> should be fixed now.\n> \n> If this still happens, please return the needinfo for Peter removed in\n> comment 30\n> (by mistake?)\n\nTested again with libvirt-3.2.0-14.el7_4.5 installed and the issue still exists"
							},
							{
								"isprivate": "0",
								"commentid": "11116945",
								"comment_count": "36",
								"who": {
									"text": "alistair.ross",
									"name": "Alistair Ross"
								},
								"bug_when": "2017-12-18 03:33:55 +0000",
								"thetext": "This issue is affecting our ability to back up any servers other than those on the same host as RHVM, so it is high priority for us. \n\nWe use Commvault 11SP9 which makes RHV API calls to make the snapshots."
							},
							{
								"isprivate": "0",
								"commentid": "11120859",
								"comment_count": "37",
								"who": {
									"text": "justin.crook",
									"name": "Justin Crook"
								},
								"bug_when": "2017-12-19 04:14:42 +0000",
								"thetext": "This issue is also affecting our ability to back up any servers other than those on the same host - currently have fully updated to 4.1.8\n\nWe use Commvault 11SP9+ which makes RHV API calls to make the snapshots.\n\nThis has now stopped our major DR project as we cannot obtain the required backups of VMs.\nThrough Commvault we have been advised that this is expected to be fixed in 4.1.9. Is there an ETA on a release date or a hotfix that can be installed yet?"
							},
							{
								"isprivate": "0",
								"commentid": "11136582",
								"comment_count": "39",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2017-12-26 23:02:55 +0000",
								"thetext": "I just found out that the audit logs are broken due to a libvirt bug. See https://github.com/ehabkost/libvirt/commit/fa7b97da69595ec4b8992ccaacfe5a7347436d6a"
							},
							{
								"isprivate": "0",
								"commentid": "11137701",
								"comment_count": "40",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2017-12-27 18:00:25 +0000",
								"thetext": "I need help from somebody familiar with VDSM to reproduce the bug. Is there anybody available this week who is able to reproduce the bug and can give me access to the host where it can be seen?"
							},
							{
								"isprivate": "0",
								"commentid": "11137763",
								"comment_count": "42",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-12-27 19:50:36 +0000",
								"thetext": "Raz, can we setup a test system for Eduardo? see comment 40."
							},
							{
								"isprivate": "0",
								"commentid": "11138997",
								"comment_count": "43",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2017-12-28 14:31:16 +0000",
								"thetext": "I confirm that this a bug on libvirt's namespace handling.  It's possible to work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf."
							},
							{
								"isprivate": "0",
								"commentid": "11139153",
								"comment_count": "44",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-12-28 16:42:46 +0000",
								"thetext": "(In reply to Eduardo Habkost from comment #43)\n> I confirm that this a bug on libvirt's namespace handling.  It's possible to\n> work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf.\n\nHow does it change libvirt/qemu behavior? What functionality is lost when using\nthis workaround?"
							},
							{
								"isprivate": "0",
								"commentid": "11139242",
								"comment_count": "45",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2017-12-28 17:25:24 +0000",
								"thetext": "(In reply to Nir Soffer from comment #44)\n> (In reply to Eduardo Habkost from comment #43)\n> > I confirm that this a bug on libvirt's namespace handling.  It's possible to\n> > work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf.\n> \n> How does it change libvirt/qemu behavior? What functionality is lost when\n> using\n> this workaround?\n\nThe libvirt namespace feature creates a separate /dev directory for QEMU to use.  The bug is in the code that handles symlinks: it assumes that an existing symlink will always point to the same target, but in the case of LVM this assumption is broken: if devices are deactivated and reactivated in a different order, the existing symlink inside QEMU's /dev directory needs to be updated to match the new path.\n\nThe feature is an additional security layer, but no functionality should be lost if disabling it.  However, the namespace feature might also help avoid races between udev and libvirt when managing devices, so some testing is recommended if changing this setting in production."
							},
							{
								"isprivate": "0",
								"commentid": "11139267",
								"comment_count": "46",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2017-12-28 17:47:51 +0000",
								"thetext": "(In reply to Eduardo Habkost from comment #45)\n> The feature is an additional security layer, but no functionality should be\n> lost if disabling it.  However, the namespace feature might also help avoid\n> races between udev and libvirt when managing devices, so some testing is\n> recommended if changing this setting in production.\n\nFor reference, this is the bug addressed by the namespace feature in libvirt:\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1404952"
							},
							{
								"isprivate": "0",
								"commentid": "11139316",
								"comment_count": "47",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2017-12-28 18:30:41 +0000",
								"thetext": "Experimental fix at:\nhttps://github.com/ehabkost/libvirt/commit/89f1a08b9518148f6a86600c0ded6f52886e44b4"
							},
							{
								"isprivate": "0",
								"commentid": "11139339",
								"comment_count": "48",
								"who": {
									"text": "nsoffer",
									"name": "Nir Soffer"
								},
								"bug_when": "2017-12-28 18:53:33 +0000",
								"thetext": "(In reply to Eduardo Habkost from comment #46)\n> For reference, this is the bug addressed by the namespace feature in libvirt:\n> https://bugzilla.redhat.com/show_bug.cgi?id=1404952\n\nVdsm already worked around this issue since 2014 by not specifying owner and\ngroup in vdsm udev rules, using chown to set the owner and group:\nhttps://gerrit.ovirt.org/33875\n\nSo we should be safe to disable libvirt namespaces, but we never tested this \nconfiguration in 7.4."
							},
							{
								"isprivate": "0",
								"commentid": "11143378",
								"comment_count": "49",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-12-31 09:21:49 +0000",
								"thetext": "(In reply to Eduardo Habkost from comment #43)\n> I confirm that this a bug on libvirt's namespace handling.  It's possible to\n> work around the bug by adding \"namespaces = [ ]\" to /etc/libvirt/qemu.conf.\n\nThe suggested W/A didn't work for me.\n1) added namespaces = [ ] to /etc/libvirt/qemu.conf\n2) # systectl restart vdsmd\n3) and performed the steps to reproduced from the original bug.\n\nIssue reproduced:\n\nengine.log:\n2017-12-31 11:15:11,928+02 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] Failed in 'HotPlugDiskVDS' method\n2017-12-31 11:15:11,945+02 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] EVENT_ID: VDS_BROKER_COMMAND_FAILURE(10,802), VDSM host_mixed_1 command HotPlugDiskVDS failed: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized\n2017-12-31 11:15:11,946+02 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] Command 'org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand' return value 'StatusOnlyReturn [status=Status [code=45, message=internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized]]'\n2017-12-31 11:15:11,946+02 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] HostName = host_mixed_1\n2017-12-31 11:15:11,946+02 ERROR [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] Command 'HotPlugDiskVDSCommand(HostName = host_mixed_1, HotPlugDiskVDSParameters:{hostId='9a41eab7-a49c-4a8e-9283-217a2d25cf94', vmId='650d5642-bc06-478e-9865-8a37551c9770', diskId='df1f057e-6450-4f82-924a-5bc2d744d3cc', addressMap='[bus=0, controller=0, unit=3, type=drive, target=0]'})' execution failed: VDSGenericException: VDSErrorException: Failed to HotPlugDiskVDS, error = internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized, code = 45\n2017-12-31 11:15:11,946+02 INFO  [org.ovirt.engine.core.vdsbroker.vdsbroker.HotPlugDiskVDSCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] FINISH, HotPlugDiskVDSCommand, log id: 60bb7b14\n2017-12-31 11:15:11,947+02 ERROR [org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] Command 'org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand' failed: EngineException: org.ovirt.engine.core.vdsbroker.vdsbroker.VDSErrorException: VDSGenericException: VDSErrorException: Failed to HotPlugDiskVDS, error = internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized, code = 45 (Failed with error FailedToPlugDisk and code 45)\n2017-12-31 11:15:11,964+02 ERROR [org.ovirt.engine.core.dal.dbbroker.auditloghandling.AuditLogDirector] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] EVENT_ID: USER_FAILED_HOTPLUG_DISK(2,001), Failed to plug disk test_Disk2 to VM test (User: admin@internal-authz).\n2017-12-31 11:15:11,965+02 INFO  [org.ovirt.engine.core.bll.storage.disk.HotPlugDiskToVmCommand] (EE-ManagedThreadFactory-engine-Thread-347874) [dc627a88-bb35-4037-bf1b-130049e1c078] Lock freed to object 'EngineLock:{exclusiveLocks='[df1f057e-6450-4f82-924a-5bc2d744d3cc=DISK]', sharedLocks='[650d5642-bc06-478e-9865-8a37551c9770=VM]'}'\n\n\nvdsm.log:\n\n2017-12-31 11:15:11,448+0200 DEBUG (jsonrpc/4) [storage.TaskManager.Task] (Task='f3e0fb64-35c3-4741-85b2-c4be9c6f565a') ref 0 aborting False (task:1002)\n2017-12-31 11:15:11,454+0200 INFO  (jsonrpc/4) [virt.vm] (vmId='650d5642-bc06-478e-9865-8a37551c9770') Hotplug disk xml: <?xml version='1.0' encoding='UTF-8'?>\n<disk device=\"disk\" snapshot=\"no\" type=\"block\">\n    <address bus=\"0\" controller=\"0\" target=\"0\" type=\"drive\" unit=\"3\" />\n    <source dev=\"/rhev/data-center/mnt/blockSD/1524036d-2ce0-47ff-9f10-f985f96c0d1a/images/df1f057e-6450-4f82-924a-5bc2d744d3cc/18e8602f-abc5-4b2e-89a5-a50bac84b5b1\" />\n    <target bus=\"scsi\" dev=\"sdd\" />\n    <serial>df1f057e-6450-4f82-924a-5bc2d744d3cc</serial>\n    <driver cache=\"none\" error_policy=\"stop\" io=\"native\" name=\"qemu\" type=\"raw\" />\n</disk>\n (vm:3638)\n2017-12-31 11:15:11,515+0200 ERROR (jsonrpc/4) [virt.vm] (vmId='650d5642-bc06-478e-9865-8a37551c9770') Hotplug failed (vm:3646)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/vdsm/virt/vm.py\", line 3644, in hotplugDisk\n    self._dom.attachDevice(driveXml)\n  File \"/usr/lib/python2.7/site-packages/vdsm/virt/virdomain.py\", line 98, in f\n    ret = attr(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/libvirtconnection.py\", line 126, in wrapper\n    ret = f(*args, **kwargs)\n  File \"/usr/lib/python2.7/site-packages/vdsm/utils.py\", line 512, in wrapper\n    return func(inst, *args, **kwargs)\n  File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 540, in attachDevice\n    if ret == -1: raise libvirtError ('virDomainAttachDevice() failed', dom=self)\nlibvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-scsi0-0-0-3' could not be initialized\n\n\nIn qemu.log I see:\n\nCould not open '/rhev/data-center/mnt/blockSD/1524036d-2ce0-47ff-9f10-f985f96c0d1a/images/df1f057e-6450-4f82-924a-5bc2d744d3cc/18e8602f-abc5-4b2e-89a5-a50bac84b5b1': Operation not permitted\n\nthis is the image I'm trying to hotplug"
							},
							{
								"isprivate": "0",
								"commentid": "11143387",
								"comment_count": "50",
								"who": {
									"text": "ratamir",
									"name": "Raz Tamir"
								},
								"bug_when": "2017-12-31 09:41:52 +0000",
								"thetext": "Ignore the last comment.\n\nI forgot to restart libvirtd service as well (thanks masayag).\n\nThe W/A worked"
							},
							{
								"isprivate": "0",
								"commentid": "11161965",
								"comment_count": "51",
								"who": {
									"text": "derez",
									"name": "Daniel Erez"
								},
								"bug_when": "2018-01-07 13:55:07 +0000",
								"thetext": "*** Bug 1531155 has been marked as a duplicate of this bug. ***"
							},
							{
								"isprivate": "0",
								"commentid": "11205128",
								"comment_count": "52",
								"who": {
									"text": "amureini",
									"name": "Allon Mureinik"
								},
								"bug_when": "2018-01-22 13:27:47 +0000",
								"thetext": "A proper fix depends on a libvirt fix that is not yet released. Pushing out to 4.1.10."
							},
							{
								"isprivate": "0",
								"commentid": "11270040",
								"comment_count": "53",
								"who": {
									"text": "mayur.indalkar",
									"name": "mayur"
								},
								"bug_when": "2018-02-09 17:25:40 +0000",
								"thetext": "Hit with same issue.\n\nnova-compute.log--->\n\ninstance: 010391e3-dc9d-419c-94b6-848239fb29eb] Failed to attach volume at mountpoint: /dev/vdc\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb] Traceback (most recent call last):\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/driver.py\", line 1252, in attach_volume\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     guest.attach_device(conf, persistent=True, live=live)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib/python2.7/site-packages/nova/virt/libvirt/guest.py\", line 309, in attach_device\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     self._domain.attachDeviceFlags(device_xml, flags=flags)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 186, in doit\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     result = proxy_call(self._autowrap, f, *args, **kwargs)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 144, in proxy_call\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     rv = execute(f, *args, **kwargs)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 125, in execute\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     six.reraise(c, e, tb)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib/python2.7/site-packages/eventlet/tpool.py\", line 83, in tworker\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     rv = meth(*args, **kwargs)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]   File \"/usr/lib64/python2.7/site-packages/libvirt.py\", line 564, in attachDeviceFlags\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb]     if ret == -1: raise libvirtError ('virDomainAttachDeviceFlags() failed', dom=self)\n2018-02-09 17:58:15.855 4716 ERROR nova.virt.libvirt.driver [instance: 010391e3-dc9d-419c-94b6-848239fb29eb] libvirtError: internal error: unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk2' could not be initialized\n\n\n\n\nqemu.log---------->\n\nCould not open '/dev/disk/by-path/ip-10.182.174.189:3260-iscsi-iqn.2017-02.com.veritas:target07-lun-3': Operation not permitted\n\n\n\nI also tried with W/A mentioned in comment #43, but still not working."
							},
							{
								"isprivate": "0",
								"commentid": "11270055",
								"comment_count": "54",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2018-02-09 17:32:08 +0000",
								"thetext": "(In reply to mayur from comment #53)\n> I also tried with W/A mentioned in comment #43, but still not working.\n\nWere libvirtd and the VMs restarted?  The workaround requires restarting libvirt and restarting the VMs after changing qemu.conf."
							},
							{
								"isprivate": "0",
								"commentid": "11271021",
								"comment_count": "55",
								"who": {
									"text": "mayur.indalkar",
									"name": "mayur"
								},
								"bug_when": "2018-02-10 06:58:32 +0000",
								"thetext": "In reply to  Eduardo Habkost  from comment #54\n\nThanks Eduardo. It worked. :-)\nEarlier I had restarted only libvirt service but did not restarted VMs.\n\nNow I have restarted both of them and it worked.\n\nBut one confusion - \nI was abled to attach Volume when I tried it for first time. This issue came when I dettached it and tried to attach it again. What may be the reason for this."
							},
							{
								"isprivate": "0",
								"commentid": "11274429",
								"comment_count": "56",
								"who": {
									"text": "ehabkost",
									"name": "Eduardo Habkost"
								},
								"bug_when": "2018-02-12 13:57:59 +0000",
								"thetext": "(In reply to mayur from comment #55)\n> But one confusion - \n> I was abled to attach Volume when I tried it for first time. This issue came\n> when I dettached it and tried to attach it again. What may be the reason for\n> this.\n\nThe bug is heavily dependent on the ordering of disk attach/detach operations in the host.  It happens when the same LVM volume is reattached to a VM, but only if the major/minor number of the underlying device-mapper file changes when the volume is reactivated+reattached."
							},
							{
								"isprivate": "0",
								"commentid": "11318710",
								"comment_count": "57",
								"who": {
									"text": "aefrat",
									"name": "Avihai"
								},
								"bug_when": "2018-02-25 12:16:59 +0000",
								"thetext": "Verified on vdsm-4.20.18-1.el7ev.x86_64 ."
							},
							{
								"isprivate": "0",
								"commentid": "11430815",
								"comment_count": "58",
								"who": {
									"text": "sbonazzo",
									"name": "Sandro Bonazzola"
								},
								"bug_when": "2018-03-29 10:58:53 +0000",
								"thetext": "This bugzilla is included in oVirt 4.2.2 release, published on March 28th 2018.\n\nSince the problem described in this bug report should be\nresolved in oVirt 4.2.2 release, it has been closed with a resolution of CURRENT RELEASE.\n\nIf the solution does not work for you, please open a new bug report."
							},
							{
								"isprivate": "0",
								"commentid": "11765621",
								"comment_count": "59",
								"who": {
									"text": "alistair.ross",
									"name": "Alistair Ross"
								},
								"bug_when": "2018-07-03 23:53:23 +0000",
								"thetext": "We are still using the namespaces workaround in /etc/libvirt/qemu.conf in 4.2.3, as this bug is stated as fixed in 4.2.2, should we remove this setting from the file now?"
							},
							{
								"isprivate": "0",
								"commentid": "11765623",
								"comment_count": "60",
								"who": {
									"text": "alistair.ross",
									"name": "Alistair Ross"
								},
								"bug_when": "2018-07-03 23:53:48 +0000",
								"thetext": "We are still using the namespaces workaround in /etc/libvirt/qemu.conf in 4.2.3, as this bug is stated as fixed in 4.2.2, should we remove this setting from the file now?"
							},
							{
								"isprivate": "0",
								"commentid": "11768381",
								"comment_count": "61",
								"who": {
									"text": "gveitmic",
									"name": "Germano Veit Michel"
								},
								"bug_when": "2018-07-04 22:41:00 +0000",
								"thetext": "(In reply to Alistair Ross from comment #60)\n> We are still using the namespaces workaround in /etc/libvirt/qemu.conf in\n> 4.2.3, as this bug is stated as fixed in 4.2.2, should we remove this\n> setting from the file now?\n\nHi Alistair,\n\nYes, the workaround is not needed anymore."
							}
						]
					}
				],
				"long_desc": [
					{
						"isprivate": "0",
						"commentid": "11240636",
						"comment_count": "0",
						"who": {
							"text": "derez",
							"name": "Daniel Erez"
						},
						"bug_when": "2018-02-01 08:53:34 +0000",
						"thetext": "Description of problem:\n\nHotplugging a disk fails due to -EPERM.\n\nVersion-Release number of selected component (if applicable):\nsystemd-219-42.el7_4.4.x86_64\nlibvirt-daemon-kvm-3.2.0-14.el7_4.5.x86_64\nvdsm-4.19.42-1.el7ev.x86_64\nkernel-3.10.0-693.11.1.el7.x86_64\nqemu-kvm-rhev-2.9.0-16.el7_4.11.x86_64\n\nSteps to Reproduce:\n1. Start guest with 4 Disks (A,B,C,D block storage)\n2. Hotunplug B, C and D\n3. Hotplug B.\n\nActual results:\nHotplug fails, qemu-kvm open(2) gets EPERM.\n\nExpected results:\nHotplug works.\n\nAdditional info:\n\n# cat qemu_open.stp \nprobe kernel.function(\"do_sys_open\").return {\n    if (pid() != 2632) \n        next\n    if ($return != -1)\n        next\n    printf(\"do_sys_open returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))\n}\n\nprobe kernel.function(\"__devcgroup_check_permission\").return {\n    if (pid() != 2632) \n        next\n    if ($return != -1)\n        next\n    printf(\"__devcgroup_check_permission returned -1, performed by %s, pid %d\\n\", execname(), pid())\n    printf(\"  %s\\n\", @entry($$parms))\n}\n\nDuring failure, we have this:\n\n# stap qemu_open.stp \n__devcgroup_check_permission returned -1, performed by qemu-kvm, pid 2632\n  type=0x1 major=0xfc minor=0x9 access=0x2\ndo_sys_open returned -1, performed by qemu-kvm, pid 2632\n  dfd=0xffffffffffffff9c filename=0x56354c7dfa20 flags=0x88800 mode=0x0\n__devcgroup_check_permission returned -1, performed by qemu-kvm, pid 2632\n  type=0x1 major=0xfc minor=0x9 access=0x6\ndo_sys_open returned -1, performed by qemu-kvm, pid 2632\n  dfd=0xffffffffffffff9c filename=0x56354c7dfa20 flags=0x8c002 mode=0x0\n\nSo __devcgroup_check_permission generates the EPERM, which propagates all the way back to sys_do_open, and qemu-kvm gets this:\n\n# strace -p 2632 -e trace=open\nstrace: Process 2632 attached\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDONLY|O_NONBLOCK|O_CLOEXEC) = -1 EPERM (Operation not permitted)\nopen(\"/rhev/data-center/00000002-0002-0002-0002-000000000010/18579785-4d47-4237-9f53-3a2344e0e475/images/21d315fe-4516-4219-b484-87f716f6c2e2/673f331c-a599-4d43-bd24-3355042b41b8\", O_RDWR|O_DIRECT|O_CLOEXEC) = -1 EPERM (Operation not permitted)\n\nHere an idea on what happens within the kernel at that point:\n\n\nqemu-kvm-2632  [000]   816.223260: funcgraph_entry:            |            may_open() {\nqemu-kvm-2632  [000]   816.223260: funcgraph_entry:            |              inode_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry:            |                __inode_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry:            |                  generic_permission() {\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry: 0.038 us   |                    get_acl();\nqemu-kvm-2632  [000]   816.223261: funcgraph_entry: 0.043 us   |                    in_group_p();\nqemu-kvm-2632  [000]   816.223261: funcgraph_exit:  0.557 us   |                  }\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry:            |                  __devcgroup_inode_permission() {\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry:            |                    __devcgroup_check_permission() {\nqemu-kvm-2632  [000]   816.223262: funcgraph_entry: 0.317 us   |                      match_exception();\nqemu-kvm-2632  [000]   816.223262: funcgraph_exit:  0.597 us   |                    }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  0.856 us   |                  }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.018 us   |                }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.339 us   |              }\nqemu-kvm-2632  [000]   816.223263: funcgraph_exit:  2.636 us   |            }\n\nqemu-kvm-2632  [000]   816.223272: sys_exit_open:        0xffffffffffffffff"
					},
					{
						"isprivate": "0",
						"commentid": "11240658",
						"comment_count": "1",
						"who": {
							"text": "pkrempa",
							"name": "Peter Krempa"
						},
						"bug_when": "2018-02-01 09:01:01 +0000",
						"thetext": "The following commit should fix the issue:\n\ncommit db98e7f67ea0d7699410f514f01947cef5128a6c\nAuthor:     Michal Privoznik <mprivozn@redhat.com>\nAuthorDate: Thu Jan 4 11:11:53 2018 +0100\nCommit:     Michal Privoznik <mprivozn@redhat.com>\nCommitDate: Mon Jan 8 09:53:48 2018 +0100\n\n    qemuDomainAttachDeviceMknodHelper: Remove symlink before creating it\n\n    https://bugzilla.redhat.com/show_bug.cgi?id=1528502\n\n    So imagine you have /dev/blah symlink which points to /dev/sda.\n    You attach /dev/blah as disk to your domain. Libvirt correctly\n    creates the /dev/blah -> /dev/sda symlink in the qemu namespace.\n    However, then you detach the disk, change the symlink so that it\n    points to /dev/sdb and tries to attach the disk again. This time,\n    however, the attach fails (well, qemu attaches wrong disk)\n    because the code assumes that symlinks don't change. Well they\n    do.\n\n    This is inspired by test fix written by Eduardo Habkost.\n\n    Signed-off-by: Michal Privoznik <mprivozn@redhat.com>\n    Reviewed-by: Andrea Bolognani <abologna@redhat.com>\n\nv3.10.0-132-gdb98e7f67"
					},
					{
						"isprivate": "0",
						"commentid": "11272615",
						"comment_count": "2",
						"who": {
							"text": "nsoffer",
							"name": "Nir Soffer"
						},
						"bug_when": "2018-02-11 22:10:14 +0000",
						"thetext": "Michal, should this be MERGED?"
					},
					{
						"isprivate": "0",
						"commentid": "11273332",
						"comment_count": "3",
						"who": {
							"text": "derez",
							"name": "Daniel Erez"
						},
						"bug_when": "2018-02-12 09:19:00 +0000",
						"thetext": "(In reply to Nir Soffer from comment #2)\n> Michal, should this be MERGED?\n\nI guess Nir meant MODIFIED :)\n\nAlso, which version of libvirt-python should we require now?\nFor libvirt-daemon-kvm it's >= 3.10.0-132, right?"
					},
					{
						"isprivate": "0",
						"commentid": "11274149",
						"comment_count": "4",
						"who": {
							"text": "mprivozn",
							"name": "Michal Privoznik"
						},
						"bug_when": "2018-02-12 12:39:02 +0000",
						"thetext": "I think this bug is only waiting for new Fedora build. Cole?"
					},
					{
						"isprivate": "0",
						"commentid": "11280680",
						"comment_count": "5",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-02-13 20:32:12 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been submitted as an update to Fedora 27. https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb"
					},
					{
						"isprivate": "0",
						"commentid": "11284061",
						"comment_count": "6",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-02-14 18:27:46 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 testing repository. If problems still persist, please make note of it in this bug report.\nSee https://fedoraproject.org/wiki/QA:Updates_Testing for\ninstructions on how to install test updates.\nYou can provide feedback for this update here: https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb"
					},
					{
						"isprivate": "0",
						"commentid": "11284830",
						"comment_count": "7",
						"who": {
							"text": "crobinso",
							"name": "Cole Robinson"
						},
						"bug_when": "2018-02-14 23:11:21 +0000",
						"thetext": "Build pushed now, sorry for the delay"
					},
					{
						"isprivate": "0",
						"commentid": "11329399",
						"comment_count": "8",
						"who": {
							"text": "nsoffer",
							"name": "Nir Soffer"
						},
						"bug_when": "2018-02-28 00:34:41 +0000",
						"thetext": "Michal, is this fix available in libvirt-3.7.0-4.fc27?"
					},
					{
						"isprivate": "0",
						"commentid": "11330382",
						"comment_count": "9",
						"who": {
							"text": "mprivozn",
							"name": "Michal Privoznik"
						},
						"bug_when": "2018-02-28 06:45:06 +0000",
						"thetext": "(In reply to Nir Soffer from comment #8)\n> Michal, is this fix available in libvirt-3.7.0-4.fc27?\n\nYes, it is."
					},
					{
						"isprivate": "0",
						"commentid": "11338841",
						"comment_count": "10",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-03-01 16:23:03 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
					}
				]
			}
		},
		{
			"bug_id": 1541444,
			"parent": true,
			"security": true,
			"title": "CVE-2018-6764 libvirt: guest could inject executable code via libnss_dns.so loaded by libvirt_lxc before init",
			"bugzilla": {
				"bug_id": "1541444",
				"alias": "CVE-2018-6764",
				"creation_ts": "2018-02-02 15:13:11 +0000",
				"short_desc": "CVE-2018-6764 libvirt: guest could inject executable code via libnss_dns.so loaded by libvirt_lxc before init",
				"delta_ts": "2019-09-29 14:31:36 +0000",
				"bug_status": "CLOSED",
				"resolution": "ERRATA",
				"keywords": "Security",
				"priority": "medium",
				"bug_severity": "medium",
				"depends_on": [
					"1542814",
					"1542815",
					"1542816",
					"1589061"
				],
				"blocked": [
					{
						"bug_id": "1541446",
						"error": "NotPermitted"
					}
				],
				"external_bugs": {
					"text": "RHSA-2018:3113",
					"name": "Red Hat Product Errata"
				},
				"long_desc": [
					{
						"isprivate": "0",
						"commentid": "11246037",
						"comment_count": "0",
						"who": {
							"text": "psampaio",
							"name": "Pedro Sampaio"
						},
						"bug_when": "2018-02-02 15:13:11 +0000",
						"thetext": "libvirt_lxc resolves a host name after the guest filesystem is mounted but before the init from it is executed. That in turn causes glibc to load libnss_dns.so and it ends up being loaded from the guest tree, making it possible for the guest to inject executable code before the host filesystem is umounted and file handles closed. That has potential security implications."
					},
					{
						"isprivate": "0",
						"commentid": "11259332",
						"comment_count": "1",
						"who": {
							"text": "ppandit",
							"name": "Prasad Pandit"
						},
						"bug_when": "2018-02-07 04:32:17 +0000",
						"thetext": "Created libvirt tracking bugs for this issue:\n\nAffects: fedora-all [bug 1542815]\n\n\nCreated mingw-libvirt tracking bugs for this issue:\n\nAffects: fedora-all [bug 1542814]"
					},
					{
						"isprivate": "0",
						"commentid": "11266210",
						"comment_count": "3",
						"who": {
							"text": "berrange",
							"name": "Daniel Berrang"
						},
						"bug_when": "2018-02-08 16:05:18 +0000",
						"thetext": "Upsptream fix is in git as:\n\ncommit 759b4d1b0fe5f4d84d98b99153dfa7ac289dd167\nAuthor: Lubomir Rintel <lkundrak@v3.sk>\nDate:   Sat Jan 27 23:43:58 2018 +0100\n\n    virlog: determine the hostname on startup CVE-2018-6764\n    \n    At later point it might not be possible or even safe to use getaddrinfo(). It\n    can in turn result in a load of NSS module.\n    \n    Notably, on a LXC container startup we may find ourselves with the guest\n    filesystem already having replaced the host one. Loading a NSS module\n    from the guest tree would allow a malicous guest to escape the\n    confinement of its container environment because libvirt will not yet\n    have locked it down."
					},
					{
						"isprivate": "0",
						"commentid": "11338846",
						"comment_count": "4",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-03-01 16:23:41 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
					},
					{
						"isprivate": "0",
						"commentid": "11668253",
						"comment_count": "5",
						"who": {
							"text": "rschiron",
							"name": "Riccardo Schirone"
						},
						"bug_when": "2018-06-04 10:29:16 +0000",
						"thetext": "Patch:\nhttps://libvirt.org/git/?p=libvirt.git;a=commit;h=759b4d1b0fe5f4d84d98b99153dfa7ac289dd167\nhttps://libvirt.org/git/?p=libvirt.git;a=commit;h=6ce3acc129bfdbe7fd02bcb8bbe8af6d13903684\nhttps://libvirt.org/git/?p=libvirt.git;a=commit;h=c2dc6698c88fb591639e542c8ecb0076c54f3dfb"
					},
					{
						"isprivate": "0",
						"commentid": "12153922",
						"comment_count": "13",
						"who": {
							"text": "errata-xmlrpc",
							"name": "errata-xmlrpc"
						},
						"bug_when": "2018-10-30 07:42:18 +0000",
						"thetext": "This issue has been addressed in the following products:\n\n  Red Hat Enterprise Linux 7\n\nVia RHSA-2018:3113 https://access.redhat.com/errata/RHSA-2018:3113"
					}
				]
			}
		},
		{
			"bug_id": 1542815,
			"security": true,
			"title": "CVE-2018-6764 libvirt: guest could inject executable code via libnss_dns.so loaded by libvirt_lxc before init [fedora-all]",
			"bugzilla": {
				"bug_id": "1542815",
				"creation_ts": "2018-02-07 04:31:58 +0000",
				"short_desc": "CVE-2018-6764 libvirt: guest could inject executable code via libnss_dns.so loaded by libvirt_lxc before init [fedora-all]",
				"delta_ts": "2018-06-26 14:42:56 +0000",
				"bug_status": "CLOSED",
				"resolution": "CURRENTRELEASE",
				"keywords": "Security, SecurityTracking",
				"priority": "medium",
				"bug_severity": "medium",
				"blocked": [
					{
						"bug_id": "1541444",
						"alias": "CVE-2018-6764",
						"creation_ts": "2018-02-02 15:13:11 +0000",
						"short_desc": "CVE-2018-6764 libvirt: guest could inject executable code via libnss_dns.so loaded by libvirt_lxc before init",
						"delta_ts": "2019-09-29 14:31:36 +0000",
						"bug_status": "CLOSED",
						"resolution": "ERRATA",
						"keywords": "Security",
						"priority": "medium",
						"bug_severity": "medium",
						"depends_on": [
							"1542814",
							"1542815",
							"1542816",
							"1589061"
						],
						"blocked": [
							{
								"bug_id": "1541446",
								"error": "NotPermitted"
							}
						],
						"external_bugs": {
							"text": "RHSA-2018:3113",
							"name": "Red Hat Product Errata"
						},
						"long_desc": [
							{
								"isprivate": "0",
								"commentid": "11246037",
								"comment_count": "0",
								"who": {
									"text": "psampaio",
									"name": "Pedro Sampaio"
								},
								"bug_when": "2018-02-02 15:13:11 +0000",
								"thetext": "libvirt_lxc resolves a host name after the guest filesystem is mounted but before the init from it is executed. That in turn causes glibc to load libnss_dns.so and it ends up being loaded from the guest tree, making it possible for the guest to inject executable code before the host filesystem is umounted and file handles closed. That has potential security implications."
							},
							{
								"isprivate": "0",
								"commentid": "11259332",
								"comment_count": "1",
								"who": {
									"text": "ppandit",
									"name": "Prasad Pandit"
								},
								"bug_when": "2018-02-07 04:32:17 +0000",
								"thetext": "Created libvirt tracking bugs for this issue:\n\nAffects: fedora-all [bug 1542815]\n\n\nCreated mingw-libvirt tracking bugs for this issue:\n\nAffects: fedora-all [bug 1542814]"
							},
							{
								"isprivate": "0",
								"commentid": "11266210",
								"comment_count": "3",
								"who": {
									"text": "berrange",
									"name": "Daniel Berrang"
								},
								"bug_when": "2018-02-08 16:05:18 +0000",
								"thetext": "Upsptream fix is in git as:\n\ncommit 759b4d1b0fe5f4d84d98b99153dfa7ac289dd167\nAuthor: Lubomir Rintel <lkundrak@v3.sk>\nDate:   Sat Jan 27 23:43:58 2018 +0100\n\n    virlog: determine the hostname on startup CVE-2018-6764\n    \n    At later point it might not be possible or even safe to use getaddrinfo(). It\n    can in turn result in a load of NSS module.\n    \n    Notably, on a LXC container startup we may find ourselves with the guest\n    filesystem already having replaced the host one. Loading a NSS module\n    from the guest tree would allow a malicous guest to escape the\n    confinement of its container environment because libvirt will not yet\n    have locked it down."
							},
							{
								"isprivate": "0",
								"commentid": "11338846",
								"comment_count": "4",
								"who": {
									"text": "updates",
									"name": "Fedora Update System"
								},
								"bug_when": "2018-03-01 16:23:41 +0000",
								"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
							},
							{
								"isprivate": "0",
								"commentid": "11668253",
								"comment_count": "5",
								"who": {
									"text": "rschiron",
									"name": "Riccardo Schirone"
								},
								"bug_when": "2018-06-04 10:29:16 +0000",
								"thetext": "Patch:\nhttps://libvirt.org/git/?p=libvirt.git;a=commit;h=759b4d1b0fe5f4d84d98b99153dfa7ac289dd167\nhttps://libvirt.org/git/?p=libvirt.git;a=commit;h=6ce3acc129bfdbe7fd02bcb8bbe8af6d13903684\nhttps://libvirt.org/git/?p=libvirt.git;a=commit;h=c2dc6698c88fb591639e542c8ecb0076c54f3dfb"
							},
							{
								"isprivate": "0",
								"commentid": "12153922",
								"comment_count": "13",
								"who": {
									"text": "errata-xmlrpc",
									"name": "errata-xmlrpc"
								},
								"bug_when": "2018-10-30 07:42:18 +0000",
								"thetext": "This issue has been addressed in the following products:\n\n  Red Hat Enterprise Linux 7\n\nVia RHSA-2018:3113 https://access.redhat.com/errata/RHSA-2018:3113"
							}
						]
					}
				],
				"long_desc": [
					{
						"isprivate": "0",
						"commentid": "11259328",
						"comment_count": "0",
						"who": {
							"text": "ppandit",
							"name": "Prasad Pandit"
						},
						"bug_when": "2018-02-07 04:31:58 +0000",
						"thetext": "\nThis is an automatically created tracking bug!  It was created to ensure\nthat one or more security vulnerabilities are fixed in affected versions\nof fedora-all.\n\nFor comments that are specific to the vulnerability please use bugs filed\nagainst the \"Security Response\" product referenced in the \"Blocks\" field.\n\nFor more information see:\nhttp://fedoraproject.org/wiki/Security/TrackingBugs\n\nWhen submitting as an update, use the fedpkg template provided in the next\ncomment(s).  This will include the bug IDs of this tracking bug as well as\nthe relevant top-level CVE bugs.\n\nPlease also mention the CVE IDs being fixed in the RPM changelog and the\nfedpkg commit message.\n\nNOTE: this issue affects multiple supported versions of Fedora. While only\none tracking bug has been filed, please correct all affected versions at\nthe same time.  If you need to fix the versions independent of each other,\nyou may clone this bug as appropriate."
					},
					{
						"isprivate": "0",
						"commentid": "11259331",
						"comment_count": "1",
						"who": {
							"text": "ppandit",
							"name": "Prasad Pandit"
						},
						"bug_when": "2018-02-07 04:32:09 +0000",
						"thetext": "Use the following template to for the 'fedpkg update' request to submit an\nupdate for this issue as it contains the top-level parent bug(s) as well as\nthis tracking bug.  This will ensure that all associated bugs get updated\nwhen new packages are pushed to stable.\n\n=====\n\n# bugfix, security, enhancement, newpackage (required)\ntype=security\n\n# testing, stable\nrequest=testing\n\n# Bug numbers: 1234,9876\nbugs=1541444,1542815\n\n# Description of your update\nnotes=Security fix for [PUT CVEs HERE]\n\n# Enable request automation based on the stable/unstable karma thresholds\nautokarma=True\nstable_karma=3\nunstable_karma=-3\n\n# Automatically close bugs when this marked as stable\nclose_bugs=True\n\n# Suggest that users restart after update\nsuggest_reboot=False\n\n======\n\nAdditionally, you may opt to use the bodhi web interface to submit updates:\n\nhttps://bodhi.fedoraproject.org/updates/new"
					},
					{
						"isprivate": "0",
						"commentid": "11280682",
						"comment_count": "2",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-02-13 20:32:48 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been submitted as an update to Fedora 27. https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb"
					},
					{
						"isprivate": "0",
						"commentid": "11284063",
						"comment_count": "3",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-02-14 18:28:03 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 testing repository. If problems still persist, please make note of it in this bug report.\nSee https://fedoraproject.org/wiki/QA:Updates_Testing for\ninstructions on how to install test updates.\nYou can provide feedback for this update here: https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb"
					},
					{
						"isprivate": "0",
						"commentid": "11338844",
						"comment_count": "4",
						"who": {
							"text": "updates",
							"name": "Fedora Update System"
						},
						"bug_when": "2018-03-01 16:23:20 +0000",
						"thetext": "libvirt-3.7.0-4.fc27 has been pushed to the Fedora 27 stable repository. If problems still persist, please make note of it in this bug report."
					}
				]
			}
		}
	],
	"builds": [
		{
			"epoch": 0,
			"nvr": "libvirt-3.7.0-4.fc27",
			"release_id": 17,
			"signed": true,
			"type": "rpm",
			"package": {
				"aarch64": [
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "aarch64"
					}
				],
				"armv7hl": [
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "armv7hl"
					}
				],
				"i686": [
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-libxl",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-vbox-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-vbox",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-xen",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-xen-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-xen",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-vbox",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-libxl-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "i686"
					}
				],
				"ppc64": [
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64"
					}
				],
				"ppc64le": [
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "ppc64le"
					}
				],
				"s390x": [
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					},
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "s390x"
					}
				],
				"src": [
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "src"
					}
				],
				"x86_64": [
					{
						"name": "libvirt-daemon-driver-lxc-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-nodedev",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-wireshark",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-interface",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-interface-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-libxl-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-login-shell-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-login-shell",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-gluster-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-secret-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-libs-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-core",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-devel",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-client",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-lock-sanlock",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-xen",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-core-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-zfs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-admin-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-uml-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-xen-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-xen",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-nss-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-client-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-secret",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-config-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-kvm",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-nss",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-uml",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-vbox",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-libxl",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-lock-sanlock-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-nodedev-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-rbd",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-vbox",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-debugsource",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-disk-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-sheepdog-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-network-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-mpath",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-wireshark-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-libs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-docs",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-lxc",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-qemu-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-logical",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-admin",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-iscsi-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-config-network",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-nwfilter",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-vbox-debuginfo",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-storage-scsi",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					},
					{
						"name": "libvirt-daemon-driver-qemu",
						"version": "3.7.0",
						"release": "4.fc27",
						"arch": "x86_64"
					}
				]
			}
		}
	],
	"comments": [
		{
			"id": 731111,
			"text": "This update has been submitted for testing by crobinso. ",
			"timestamp": "2018-02-13 20:32:09",
			"update_id": 107301,
			"user": {
				"avatar": "https://apps.fedoraproject.org/img/icons/bodhi-24.png",
				"id": 91,
				"name": "bodhi",
				"openid": "bodhi.id.fedoraproject.org"
			},
			"user_id": 91
		},
		{
			"id": 731683,
			"text": "This update has been pushed to testing.",
			"timestamp": "2018-02-14 18:29:18",
			"update_id": 107301,
			"user": {
				"avatar": "https://apps.fedoraproject.org/img/icons/bodhi-24.png",
				"id": 91,
				"name": "bodhi",
				"openid": "bodhi.id.fedoraproject.org"
			},
			"user_id": 91
		},
		{
			"id": 731762,
			"karma": 1,
			"text": "works for me",
			"timestamp": "2018-02-14 22:00:21",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/054959f4db5d1ad8f22cae1ba73e26ebfd3edd6690c21cc7ee00608f294ff22a?s=24&d=retro",
				"email": "cserpentis@gmail.com",
				"groups": [
					{
						"name": "packager"
					},
					{
						"name": "ipausers"
					},
					{
						"name": "fedora-contributor"
					},
					{
						"name": "signed_fpca"
					},
					{
						"name": "fedorabugs"
					}
				],
				"id": 739,
				"name": "cserpentis",
				"openid": "cserpentis.id.fedoraproject.org"
			},
			"user_id": 739
		},
		{
			"id": 732284,
			"karma": 1,
			"karma_critpath": 1,
			"text": "No regressions noted.",
			"timestamp": "2018-02-16 00:18:05",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/e62cb974458dc33fcf2225462b8765e0aadcbbd698560c8f8dad6c3e23d5e47e?s=24&d=retro",
				"email": "dhgutteridge@hotmail.com",
				"groups": [
					{
						"name": "ipausers"
					}
				],
				"id": 194,
				"name": "dhgutteridge",
				"openid": "dhgutteridge.id.fedoraproject.org"
			},
			"user_id": 194
		},
		{
			"id": 732313,
			"karma": 1,
			"text": "Works for me.",
			"timestamp": "2018-02-16 05:26:57",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/784d01fedc3b012e7672c3c17ce329d7ecdbfa47d850fb87180fab60647f2f49?s=24&d=retro",
				"email": "thebeardedhermit@hotmail.com",
				"id": 2713,
				"name": "sassam",
				"openid": "sassam.id.fedoraproject.org"
			},
			"user_id": 2713
		},
		{
			"id": 732951,
			"karma": 1,
			"text": "no issues",
			"timestamp": "2018-02-18 18:21:56",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/141fe885554225ace2a85de2e3509335c9debe0a1d6caed953b6767a4c9c77aa?s=24&d=retro",
				"email": "amcg@thesignal.eu",
				"id": 3450,
				"name": "amcg",
				"openid": "amcg.id.fedoraproject.org"
			},
			"user_id": 3450
		},
		{
			"id": 733058,
			"karma": 1,
			"text": "wfm",
			"timestamp": "2018-02-19 01:59:46",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/cd8c7a6c7915ec305684dc7062ae860ea8d43040037d3bea3d20e03d2621566a?s=24&d=retro",
				"email": "williamjmorenor@gmail.com",
				"groups": [
					{
						"name": "packager"
					}
				],
				"id": 216,
				"name": "williamjmorenor",
				"openid": "williamjmorenor.id.fedoraproject.org"
			},
			"user_id": 216
		},
		{
			"id": 733242,
			"karma": 1,
			"text": "Works for me.",
			"timestamp": "2018-02-19 17:36:35",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/fbb28238d8e88f4a6ca70e8409ab425f0139e2596b4f9c411d1da55f4c4fe3c1?s=24&d=retro",
				"email": "mhayden@redhat.com",
				"groups": [
					{
						"name": "packager"
					},
					{
						"name": "ipausers"
					},
					{
						"name": "fedora-contributor"
					},
					{
						"name": "signed_fpca"
					},
					{
						"name": "fedorabugs"
					},
					{
						"name": "ambassadors"
					},
					{
						"name": "gitfedora-security-team"
					},
					{
						"name": "aws-fedora-ci"
					},
					{
						"name": "cloud-sig"
					}
				],
				"id": 536,
				"name": "mhayden",
				"openid": "mhayden.id.fedoraproject.org"
			},
			"user_id": 536
		},
		{
			"id": 733562,
			"karma": 1,
			"text": "no regressions noted",
			"timestamp": "2018-02-20 01:29:49",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/d826994e83ce867281f73d85986dd475938841874b75a83a82cdb184f6af8749?s=24&d=retro",
				"email": "rosset.filipe@gmail.com",
				"groups": [
					{
						"name": "proventesters"
					},
					{
						"name": "packager"
					},
					{
						"name": "provenpackager"
					},
					{
						"name": "ipausers"
					},
					{
						"name": "fedora-contributor"
					},
					{
						"name": "signed_fpca"
					},
					{
						"name": "fedorabugs"
					},
					{
						"name": "ambassadors"
					},
					{
						"name": "l10n"
					},
					{
						"name": "cvsl10n"
					},
					{
						"name": "fedora-br"
					}
				],
				"id": 124,
				"name": "filiperosset",
				"openid": "filiperosset.id.fedoraproject.org"
			},
			"user_id": 124
		},
		{
			"id": 735234,
			"karma": 1,
			"text": "my VMs work fine",
			"timestamp": "2018-02-23 12:37:09",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/dcc9b804c7f4931d90ad5f9b8b2518e79df4743a418f53589057dc035bddcd0d?s=24&d=retro",
				"email": "kparal@redhat.com",
				"groups": [
					{
						"name": "qa-tools-sig"
					},
					{
						"name": "proventesters"
					},
					{
						"name": "qa"
					},
					{
						"name": "packager"
					},
					{
						"name": "ipausers"
					},
					{
						"name": "fedora-contributor"
					},
					{
						"name": "signed_fpca"
					},
					{
						"name": "fedorabugs"
					},
					{
						"name": "qa-admin"
					}
				],
				"id": 411,
				"name": "kparal",
				"openid": "kparal.id.fedoraproject.org"
			},
			"user_id": 411
		},
		{
			"id": 735546,
			"karma": 1,
			"text": "Works just fine",
			"timestamp": "2018-02-24 09:08:03",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/f80c96a3994c2479fc4815a146079bf93ade230fe3f3993213d2b8222cafc73d?s=24&d=retro",
				"email": "fzatlouk@redhat.com",
				"groups": [
					{
						"name": "packager"
					},
					{
						"name": "qa-tools-sig"
					},
					{
						"name": "provenpackager"
					},
					{
						"name": "fedora-contributor"
					},
					{
						"name": "qa-admin"
					},
					{
						"name": "signed_fpca"
					},
					{
						"name": "sysadmin-qa"
					},
					{
						"name": "sysadmin"
					},
					{
						"name": "ambassadors"
					},
					{
						"name": "advocates"
					},
					{
						"name": "fedorabugs"
					},
					{
						"name": "qa"
					},
					{
						"name": "ipausers"
					}
				],
				"id": 2240,
				"name": "frantisekz",
				"openid": "frantisekz.id.fedoraproject.org"
			},
			"user_id": 2240
		},
		{
			"id": 737581,
			"karma": 1,
			"karma_critpath": 1,
			"text": "This works on my Vagrant host.",
			"timestamp": "2018-02-28 15:41:39",
			"update_id": 107301,
			"user": {
				"avatar": "https://seccdn.libravatar.org/avatar/2def3c902372f3cdad2450f86481f1cd1632419cce90220c18ea9da06c7bca24?s=24&d=retro",
				"email": "randy@electronsweatshop.com",
				"groups": [
					{
						"name": "packager"
					},
					{
						"name": "provenpackager"
					},
					{
						"name": "ipausers"
					},
					{
						"name": "fedora-contributor"
					},
					{
						"name": "signed_fpca"
					},
					{
						"name": "fedorabugs"
					},
					{
						"name": "communishift"
					},
					{
						"name": "atomic-wg"
					},
					{
						"name": "erlang"
					},
					{
						"name": "erlang-maint-sig"
					}
				],
				"id": 2897,
				"name": "bowlofeggs",
				"openid": "bowlofeggs.id.fedoraproject.org"
			},
			"user_id": 2897
		},
		{
			"id": 737595,
			"text": "This update has been submitted for stable by crobinso. ",
			"timestamp": "2018-02-28 16:38:49",
			"update_id": 107301,
			"user": {
				"avatar": "https://apps.fedoraproject.org/img/icons/bodhi-24.png",
				"id": 91,
				"name": "bodhi",
				"openid": "bodhi.id.fedoraproject.org"
			},
			"user_id": 91
		},
		{
			"id": 738029,
			"text": "This update has been pushed to stable.",
			"timestamp": "2018-03-01 16:24:24",
			"update_id": 107301,
			"user": {
				"avatar": "https://apps.fedoraproject.org/img/icons/bodhi-24.png",
				"id": 91,
				"name": "bodhi",
				"openid": "bodhi.id.fedoraproject.org"
			},
			"user_id": 91
		}
	],
	"content_type": "rpm",
	"critpath": true,
	"date_pushed": "2018-03-01 16:22:58",
	"date_stable": "2018-03-01 16:22:58",
	"date_submitted": "2018-02-13 20:32:09",
	"date_testing": "2018-02-14 18:26:40",
	"karma": 10,
	"meets_testing_requirements": true,
	"notes": "* CVE-2018-5748: resource exhaustion via qemuMonitorIORead() (bz #1535785)\n* CVE-2018-6764: code injection via libvirt_lxc (bz #1542815)\n* Fix hotplug disk failure (bz #1540872)",
	"pushed": true,
	"release": {
		"branch": "f27",
		"candidate_tag": "f27-updates-candidate",
		"composed_by_bodhi": true,
		"dist_tag": "f27",
		"id_prefix": "FEDORA",
		"long_name": "Fedora 27",
		"mail_template": "fedora_errata_template",
		"name": "F27",
		"override_tag": "f27-override",
		"package_manager": "dnf",
		"pending_signing_tag": "f27-signing-pending",
		"pending_stable_tag": "f27-updates-pending",
		"pending_testing_tag": "f27-updates-testing-pending",
		"stable_tag": "f27-updates",
		"state": "archived",
		"testing_repository": "updates-testing",
		"testing_tag": "f27-updates-testing",
		"version": "27"
	},
	"require_bugs": true,
	"require_testcases": true,
	"severity": "unspecified",
	"stable_karma": 3,
	"status": "stable",
	"suggest": "unspecified",
	"test_gating_status": "passed",
	"title": "libvirt-3.7.0-4.fc27",
	"type": "security",
	"url": "https://bodhi.fedoraproject.org/updates/FEDORA-2018-b22d46eabb",
	"unstable_karma": -3,
	"updateid": "FEDORA-2018-b22d46eabb",
	"user": {
		"avatar": "https://seccdn.libravatar.org/avatar/2db9913ea72f8be4fb00bf87f927f22b40a6c3b348f68abbabdf4d1a93ff09af?s=24&d=retro",
		"email": "crobinso@redhat.com",
		"groups": [
			{
				"name": "packager"
			},
			{
				"name": "provenpackager"
			},
			{
				"name": "virtmaint-sig"
			},
			{
				"name": "ipausers"
			},
			{
				"name": "fedora-contributor"
			},
			{
				"name": "signed_fpca"
			},
			{
				"name": "fedorabugs"
			},
			{
				"name": "hosted-content"
			},
			{
				"name": "kubevirt"
			}
		],
		"id": 400,
		"name": "crobinso",
		"openid": "crobinso.id.fedoraproject.org"
	},
	"version_hash": "fd0419fcdc2c8c2b7ac21f22becdfa12322509f3"
}
